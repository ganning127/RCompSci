%%% Research Diary - Entry
%%% Template by Mikhail Klassen, April 2013

\documentclass[11pt,letterpaper]{article}

\newcommand{\workingDate}{\textsc{2022 $|$ January $|$ 31}}
\newcommand{\userName}{Ganning Xu}
\newcommand{\institution}{Research in \\ Computational Science \\ North Carolina School of Science and Math}
\usepackage{researchdiary_png}
%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={((},close={))}}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\par\singlespacing\small}

\begin{comment}
Resources
    Table Generator: https://www.tablesgenerator.com/
    Overleaf Project: https://www.overleaf.com/project/61fc1906472d003c89b4c1f1
    GitHub Project: https://github.com/ganning127/RCompSci_Research_Notebook
\end{comment}

\begin{document}
\univlogo

\section{Week of January 31, 2022}

$\surd$ {\Large \textcolor{red}{RRG Comment:acknowledging notebook establishment } } 

\textbf{Note}: I take a lot of my notes in my \href{https://drive.google.com/drive/folders/17ZhkJZveiDomM2K2MYST_-PhuzrndiVT?usp=sharing}{Google Drive}, so my notebook might not be as detailed.

\subsection{Monday, January 31, 2022}
First day of research in computational science. Went over introductions, course expectations, and how this class would work.

\subsection{Wednesday, February 2, 2022}
Went over how to read request for proposals (RFP) and covered the Research in Computational Science \href{https://drive.google.com/file/d/1cvPQnz40H3bqiEyOdgzRMyYjtK4gxPB5/view?usp=sharing}{RFP}.


We also covered an intro \href{https://drive.google.com/file/d/1hlGEsI95i6WEo5NzAbqjr73ax-MP5L8G/view?usp=sharing}{guide to computational thinking}. Begin by starting with a research question that can be broken down into subproblems. The first subproblem should be "low hanging fruit", while the latter ones should be harder to achieve. Also, it is important to make assumptions when creating the model, as it simplifies it. It's important to be able to distinguish which data is important and which data is not as useful. 

When creating algorithms, you can either use an existing one, modify an existing one, or create your own. Creating your own algorithm is usually difficult. 

\textbf{Important notes}
\begin{itemize}
    \item Letter of intent due on Friday, March 4, 2022 at 5 PM. 
    \item Preliminary proposal due on Friday, March 25, 2022, at 5 PM. 
    \item Full proposal due Wednesday, April 20, at 5 PM. 
\end{itemize}

\subsection{Thursday, February 3, 2022}
Learned \LaTeX 

Table \ref{tab:ideas} shows a table of some of my ideas for my research work.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Topic idea} & \textbf{\begin{tabular}[c]{@{}c@{}}Technology\\ Computational "X"\end{tabular}} & \textbf{Techniques and Tools} \\ \hline
Heart disease detection algorithm & Machine Learning & TensorFlow, Keras, Kaggle \\ \hline
Long text simplification & NLP & GTP3 and Kaggle \\ \hline
Factors of a successful election & Machine Learning & TensorFlow, Keras, Large dataset \\ \hline
\end{tabular}
\caption{Table of CT Ideas}
\label{tab:ideas}
\end{center}
\end{table}

\subsection{Friday, February 4, 2022}

\subsubsection * {How Science Works}
Today we learned about how science works from the national level to a researcher when they submit their RFP. Here is a link to my \href{https://docs.google.com/document/d/17-9rgkjDyZuy95PmMjWT7NfPlYd96HXTcVR5unhhW_A/edit?usp=sharing}{notes}. 

Basically, the flow of power works like this: NSB $\rightarrow$ NSF $\rightarrow$ Directorates $\rightarrow$ Groups $\rightarrow$ RFP. 

Once a researcher sends in an RFP:
\begin{enumerate}
    \item The RFP is sent to a specific project officer (typically university faculty on loan)
    \item The Project Officer will then create a certain number of review teams (~5 in team team), to review ~20 proposals.
    \item Each person in a review team scores each proposal (excellent, very good, good, fair, poor)
    \item After individual review, each review team goes to Washington DC and works on reviewing proposals for ~3 days in a review panel together
    \item Each proposal is ranked into (High, medium, and low) priority for funding. This recommendation goes to the NSF project officer, who makes a decision on who gets the money. They can overrule a panel.
\end{enumerate}

When an RFP is approved, the person in charge of the research is the Principal Investigator (PI). Each PI usually has CoPI's, unless the project is really small. 

\subsubsection * {Example RFP}
Today, we also looked over an example RFP that Mr. Gotwals submitted. It is important to note that the project summary can only be one page long and the project description has to be less than 15 pages. 

\subsection{Saturday, February 5, 2022}
Today I read over the example proposal that Mr. Gotwals gave us and completed the reading check that covered the podcast and the Intro to Computational Thinking chapter. I also started on the content from the proposal template today.

\subsection{Sunday, February 6, 2022}
Read over \href{https://drive.google.com/file/d/1weJUCV1x84bmy6dvqFjRlOviiD8dVDjg/view?usp=sharing}{GotwalsGuideCT.pdf}. Also read over \href{https://drive.google.com/file/d/1p4ew41jI2WH81WCaB8ADkFOrTVsac7Wl/view?usp=sharing}{How to summarize a research paper}.

\section{Week of February 7, 2022}
\subsection{Monday, February 7, 2022}
Learned about how spreadsheets worked in class. Completed three spreadsheet labs from the Bohr model, Gaussian Distribution, and Diatomic molecules. 

\subsection{Tuesday, February 8, 2022}
Corrected one of the spreadsheet labs.

\subsection{Wednesday, February 9, 2022}
\href{https://www.ncssm.edu/library}{ncssm.edu/library} is a good source for research articles. The \href{https://ncssm.follettdestiny.com/common/servlet/presenthomeform.do?l2m=Home&tm=Home&l2m=Home}{full catalog} has acess to journals.

\begin{itemize}
    \item HeinOnline - Legal
    \item Liebert Medical Journals - Medical Research
    \item JSTOR - Humanities
    \item NC Live - Biggest DB (UNC System, private colleges, community colleges). Can access the NYTimes and Washington post through here
\end{itemize}

\textbf{Ways to read a research paper}
\begin{enumerate}
    \item Read abstract and conclusions. If important can look at the entire thing. 
    \item The last sentence of the introduction usually contains the RQ
    \item Look at references of research paper first (see if you see a certain name that is repeated, they are the expert)
    \item Read many papers from the expert and see if you can reach out to them for them to be your mentor
    \item Keep the \href{https://www.mendeley.com/reference-manager/}{citation manager} UPDATED!
\end{enumerate}

\textbf{Types of Research}
\begin{itemize}
    \item Literature Review: A researcher goes out and finds research papers about a topic over a certain period of time. This is very good to find if you can. 
\end{itemize}


\subsection{Thursday, February 10, 2022}
Today I read over the research paper: Recent Approaches for Text Summarization Notes, and created a draft of my J-Club presentation on the topic. There are two main types of text summarization: abstractive and extractive. Abstractive summarization focuses on semantic understanding the text and re-expressing that understanding in easier words. Extractive summarization focuses on removing unnecessary sentences and words to create a summary sentence with parts of existing ones.

\subsection{Friday, February 11, 2022}
Today was the Mathematica lab, where we learned how to perform text analysis on a speech that was 80,000 characters long. I also made edits to my J-Club draft today.

\subsection{Saturday, February 12, 2022}
I was at the NCHSAA States Swim meet for today, so I didn't get a chance to work on RCompSci.

\subsection{Sunday, February 13, 2022}
Today I read over my research paper for JClub again and took more notes on it. I think that I have a good idea of how I'm going to do my JClub presentation.

\section{Week of February 14, 2022}
\subsection{Monday, February 14, 2022}
Today I worked on my JClub presentation: Shortening sentences, including graphics with captions, and making sure that I truly understood everything that I will be talking about. However, this was difficult as the paper I read had some complicated math in it and a LOT of grammatical errors. I finished the draft of my JClub presentation today.

Today I also created a script that I will be using to practice for my JClub presentation.

\subsection{Tuesday, February 15, 2022}
I read over the "Top60QuestionsFrequentlyAskedDuringThesisDefense.pdf" and "rocreguant.com-What questions to ask after a scientific presentation.pdf" files on Canvas. While asking questions may seem like you're hassling the researcher, in reality, you're actually helping them see their own project from another perspective, something different from what they're used to. Some good questions:

Make sure to always be polite when asking questions.
\begin{itemize}
    \item What is next for your research project?
    \item The results of your findings seem promising, but they contradict X, why is this?
\end{itemize}

Additionally, I learned that you should never actually show the true weaknesses of the research project you are "defending". You don't want to give the other side any doubts of uncertainty. 

Today I also made edits to my JClub presentation and edited my script to make sure that there were no typos.

\subsection{Wednesday, February 16, 2022}
Today I read over the ATE RFP and a sample proposal and completed the proposal review guide. I realized that:
\begin{itemize}
    \item It is very important to read a RFP thoroughly before starting to write a proposal
    \item Intellectual merit = how does this further what we already know 
    \item Broader impacts = how can this research be applied to different subjects and increased the diversity of the scientific community.
\end{itemize}

\subsection{Thursday, February 17, 2022}
Today I edited and finished my review of a sample proposal. Today I also read over the paper 
"Classifier-Based Text Simplification for Improved Machine Translation". 

I took notes on this and my notes are linked \href{https://docs.google.com/document/d/1CNj_tqSD1VNpwqlWfZUubP9V462ZieM0riJjaL6dITU/edit?usp=sharing}{here}. I think that my RQ 1     will be figuring out how to add explanations to difficult words, such as: 

\textbf{Original}: Pulmonary atresia

\textbf{Simplified}: Pulmonary atresia (a type of birth defect) 

\subsection{Friday, February 18, 2022}
In class today we had a panel to determine the priority of funding for a NSF proposal. Many of us were "reviewer 2", reviewing the proposal very harshly.

Today I read over the first couple of pages of the \href{https://aclanthology.org/D17-1062.pdf}{"Sentence Simplification with Deep Reinforcement Learning"}.

I also watched the video \href{https://www.youtube.com/watch?v=0X4zlwXujco}{"https://www.youtube.com/watch?v=0X4zlwXujco"}

Also \href{https://zbib.org/}{here} is a good citation maker. 

\subsection{Saturday, February 19, 2022}
Today I finished reading the paper "Sentence Simplification with Deep Reinforcement Learning". 

Elaboration on \textbf{RQ1}:
\begin{itemize}
    \item Software that allows you to feed it a complex sentence and simplifies it by adding annotations (in parenthesis) of hard to define terms
    \item API endpoint/NPM package could be created for this for a sentence/passage to be sent and a annotated version is returned.
\end{itemize}

\textbf{Example}:
"Drosophila melanogaster is a very annoying bug" $\rightarrow$ "Drosophila melanogaster (a type of fruit fly) is a very annoying bug"

I also practiced my JClub presentation today.

\subsection{Sunday, February 20, 2022}
Today I practiced my presentation for JClub, going through and fixing errors that I had. The time I got was 9 minutes. I need to learn more about hidden markov models and how they work though, because I don't fully understand it. 

\section{Week of February 21, 2022}

\subsection{Monday, February 21, 2022}
Today I edited my JClub presentation to reduce the amount of text and increase the amount and size of each image. I also watched a video that walked through creating a text summary using Spacy. However, this video chose summary sentences based on picking out sentences with the most occurances of the most common term throughout all of the text. I think that a possible RQ2 might be to extractive summary, but based on more information than just the most common word. I think that RQ 3 would be developing an algorithm that is able to do abstractive summary. Video link: https://www.youtube.com/watch?v=9PoKellNrBc

\subsection{Tuesday, February 22, 2022}
Today I read the first part of the research paper "Source sentence simplification for statistical machine translation". This paper goes over whether or not actually using simplified sentences is beneficial to translations as compared to more complex sentences. 

\subsection{Wednesday, February 23, 2022}
Today I practiced my JClub presentation and watched the video: "How to Make a Text Summarizer - Intro to Deep Learning #10". GloVe is a Python module that uses abstractive summarization to create headlines for articles based on their content. I also skimmed over the paper that I started reading yesterday, and realized that I don't really understand some parts of what I'm reading, so I need to investigate individual smaller topics first.

\subsection{Thursday, February 24, 2022}
Today I read over the paper "Text summarization using unsupervised deep learning". Most research papers I've read focus on either one of two different types of text simplification: extractive simplification and abstractive simplification. However, I wonder if there's another method of simplification that merges the two methods, by cuttong out some parts but also by adding some new material? 

\subsection{Friday, February 25, 2022}
On Friday I created a text summarization method in Python that uses cosine similarity between different sentences. Cosine similarity determines how different two sentences are by comparing the angle between their vectors. The article that I followed to create this uses extractive summarization to create the final summary. However, the grammar and verb tenses weren't the best. Also, the sentences weren't complete in the summarized version.

\subsection{Saturday, February 26, 2022}
Today I read over the article "A  Gentle Introduction to Text Summarization in Machine Learning", which included code and general steps in creating a summarization of a long text. The general process of creating an extractive summarization works like this:

\begin{enumerate}
    \item Convert paragraph into sentences by splitting on period and a space (so decimals don’t get counted)
    \item Text processing (removing stopwords)
    \item Tokenization (convert each sentence into a list of words)
    \item Evaluate the weighted occurrence frequency in words (Weight = word occurance / occurrence of the most frequently occurring word in sentence)
    \item Substitute each of the words found in the original sentences with their weighted frequencies. Then, we’ll compute their sum (The higher sum, the better the sentence is a summarization sentence. 
)
\end{enumerate}

\subsection{Sunday, February 27, 2022}
Today I read over the paper "Text Simplification Tools: Using Machine Learning to Discover Features that Identify Difficult Text". This was one of the best papers that I have read. The \textbf{specificity or ambiguity of a word is a VERY good indicator on how complex the sentence is.}

I have an idea of my RQ 2, which might be to create a classifier that is able to predict whether or not a sentence is difficult or simple. Tying onto this RQ3 might be a classifier that is subject specific (such as biology papers or CS papers), that is able to determine the age group needed to read a research paper. This would need to be better than existing ways to find reading levels though, such as the Flesch-Kincaid grade level formulas.

Another idea might be to create a machine learning model that analyzes the reading level of a piece of text and only simplifies certain sentences if they are above a certain reading level. We would let users choose the level they want to simplify the text down to.

\section{Week of February 28, 2022}

\subsection{Monday, February 28, 2022}
Today I watched the video: "Exploring Neural Text Simplification Models", in which Sergiu Nisioi gave a talk on text simplification.

\begin{itemize}
    \item Investigated whether r not generic word representations improve simplification models.
    \item Used global embeddings from Google News (CBOW)
    \item Used regular wikipedia as input data (complex) and simplified wikipedia as desired output data (simple)
    \item Used a neural network
    \item Had human annotator determine the accuracy of the model (if the sentence was gramatically correct, accurate, info lost)
    \item However, it is difficult to determine what is a "good" simplification
    \item Code: https://github.com/senisioi/NeuralTextSimplification
\end{itemize}

\subsection{Tuessday, March 1, 2022}
Today I chose the paper that I will be using for JClub 2: \href{https://arxiv.org/pdf/1906.04165.pdf}{Leveraging BERT for Extractive Text Summarization on Lectures}. This paper focuses on using BERT to create newer methods of extractive text simplification when dealing with recorded lecture transcripts from massive open online courses (MOOCs). In many MOOCs, valuable information is hard to locate in video transcripts. Additionally, the code the author used was avaliable within the paper as well. The link to the GitHub is \href{https://github.com/dmmiller612/lecture-summarizer}{here}.


\subsection{Wednesday, March 2, 2022}
Today I continued reading my JClub article I chose. I am also taking note on the article and trying to prepare what I will be saying/doing during my JClub presentation.

\subsection{Thursday, March 3, 2022}
I finished reading my JClub paper and taking notes on it today. I didn't know that an API endpoint already existed that allowed users to send their text over and get a simplification. However, they did mention a feature for users to manage their own summaries, which is something that I might be able to do. However, they did not provide clarifying parenthesis for their difficult words, so I have them beat there (lol).

Also, they are using BERT for their text summarization model. I will be creating my JClub presentation tomorrow.

\subsection{Friday, March 4, 2022}
Today I worked on my JClub presentation. I have a rough idea on how I will be laying out my journal club, and have created slides for them. I also have a vague idea on what I will be saying on each slide.

\subsection{Saturday, March 5, 2022}
Today I worked on the QTL CompBio lab, but I have a few questions about the R Script that I was writing. I will set up a meeting with Mr. Gotwals later about this. I also read a couple of interesting articles on text summarization, and I think K-Means clustering after transforming a sentence into a vector is a REALLY good way of finding summarization sentences.

\subsection{Sunday, March 6, 2022}
I finished my first draft of JClub 2's presentation and script. I will be practice presenting this tomorrow and turning it in tomorrow.

\section{Week of March 7, 2022}
\subsection{Monday, March 7, 2022}
Today I finished my JClub 2 presentation. I practiced presenting it twice and I was exactly at the time limit. I'm struggling to find ways that I can be unique because it seems like everything has been done before. Even the API idea, which I thought was novel

\subsection{Tuesday, March 8, 2022}
I practiced my JClub presentation many times today. I am ready to present in front of the group tomorrow.

\subsection{Wednesday, March 9, 2022}
Today I presented my JClub 2 presentation on using BERT to do lecture summarization. I thought it went really well. I answered all the questions that the audience asked me. However, I was nervous during the presentation and stutterd a bit. I also read over the paper "Text Simplification Using Neural Machine Translation". There are many methods to do simplification. 

\textbf{Lexical simplification}: Simplifies text by substitution rare and difficult words with common ones
\begin{itemize}
    \item identification of difficult words
    \item finding synonyms or similar words by various similarity measures
    \item ranking and selecting the best candidate word based on criteria such as language model
    \item keeping grammar of sentence correct
\end{itemize}

\textbf{Rule based systems}
\begin{itemize}
    \item Use handcrafted rules for syntactic simplification.
    \item ranking and selecting the best candidate word based on criteria such as language model
    \item If a long sentence contains “not only, but also”, those sentences could be spilt into two sentences.
\end{itemize}

\textbf{Machine Translation}
\begin{itemize}
    \item Original English and simplified English can be thought of as two different languages
    \item Uses English and simple Wikipedia to train the model
\end{itemize}

\subsection{Thursday, March 10, 2022}
I read over the artcile "How to keep a lab notebook ScienceAAAS.pdf", which was on Canvas. Highlights:
\begin{itemize}
    \item date, time location protocol parameters, where the data is stored
    \item take notes in your lab notebook the way you want them
    \item lab notebooks can prove that you came up with the idea before someone else, so it's work keeping even if its a bit of extra work each day.
\end{itemize}

I also wrote most of the intellectual merit, RQ 1, 2, and 3, dissemination plan, and came up with a title for my proposal today. I also worked on the biographical sketch today, finishing the basic information, professional preparation, and appointments sections.

\subsection{Friday, March 11, 2022}
Today I read over and took notes on the article "Opinion mining from online hotel reviews – A text summarization approach". I think that either RQ2 or 3 can focus on multi-text summarization, where a single summary is created from multiple documents. Conflicting opinions could be both included, or the one with the most support could be added. Most support could be determined by web scraping, or the amount of evidence each author provides.

\subsection{Saturday, March 12, 2022}
I am at the FRC district competition at East Carolina University today and tomorrow. No progress today.

\subsection{Sunday, March 13, 2022}
Today I created the topics for my lightning talk. I think that a vague idea of my three research questions will be something like:

\begin{itemize}
    \item RQ1: How can a publicly accessible method of difficult text replacement be created?
    \item RQ2: How can extractive text summarization be created for research papers?
    \item RQ3: How can an abstractive text summarization method be used to create the abstract of a research paper?
\end{itemize}


\section{Week of March 14, 2022}
\subsection{Monday, March 14, 2022}
Today I identified mentors and gave my lightning talk. I added around 10 mentors to the mentor identification form, but many of them don't work in a lab and/or I didn't find their email online. Also, I edited my resume to fit with RCompSci.

\subsection{Tuesday, March 15, 2022}
Today I watched video on how BERT worked. Video: \href{https://www.youtube.com/watch?v=OR0wfP2FD3c}{BERT Explained!
} 

\begin{itemize}
    \item Uses bidirectional context to mask words and predict them
    \item BERT has two different models, base and large. 
    \item BERT base has 12 transformer blocks, compared to large's 24
    \item BERT base has a hidden dimension embedding of 768, while large has 1024.
    \item BERT base has 12 attention heads compared to 16 in large. 
    \item BERT was trained on the BooksCorpus (800 million words) and the English Wikipedia dataset (2.5 billion words)
\end{itemize}

\subsection{Wednesday, March 16, 2022}
Today I created an email draft template that I will be sending to all mentors. I also was emailing Pierre-Nicolas, who is a former RCompSci student who is currently at UNC to talk about his working using NLP. We're going to have a meeting on Thursday (3/17).

\subsection{Thursday, March 17, 2022}
Today I met with Pierre-Nicolas about my RCompSci project. I learned so much from this meeting. He said that my original RQ1 felt "out of place" when in relation with RQ2 and RQ3. I'm going to change my current RQ2 to be RQ1 and my RQ3 to be RQ2. I also need to narrow down my summarization topic. For example, the first iteration of models for extractive (RQ2) and abstractive summarization (RQ3) might only be focused on NLP papers. I always thought that abstractive simplification would be really hard

I also need to finish my RQ1 (extractive) before SRIP begins (what Pierre told me). This way, over the summer, I can work on RQ2 and RQ3 and have time to write my paper. I'm also able to show my mentor what I've completed so far and that I'm serious about this program and have a higher likelihood of them saying yes to mentor me. 

There are also a LOT of people and mentors at UNC that do this type of work. Shashank Srivastava (ssrivastava@cs.unc.edu), who is Pierre's professor, does work with abstractive text simplification. The website https://nlp.cs.unc.edu/ has A TON of people who are doing this type of work. I think I will be reaching out to many of these people and asking if they would be willing to help me on my project.

The hard part about my project is also getting the data. Many of these datasets aren't directly published, so I'll need to email the authors of these research papers that I've been reading. Pierre said that it is quite likely that they respond. 

I think my new RQ3 might still deal with dissemination of the first 2 RQs, such as creating a web app that allows users to do text simplification, OR it might be creating a NLP model that is able to work with all types of research papers, which is very broad. 

\subsection{Friday, March 18, 2022}
Today I completed the Data Science lab in class. I think that some other RQ3's might be: how can a simplification method be created that is applicable to all domains, not just including natural language processing papers. I'm also wondering how a machine learning model will deal with images and equations in a paper. Does the machine learning model just cut them out? If the equations and images are left in there though, the final "summary" paper will be longer. I think the best approach might be to cut those items out, because a summary is not meant to be fully comprehensive, its only meant to give a brief overview of the information.

\subsection{Saturday, March 19, 2022}
No progress today. I was in Asheville with my family over the extended weekend.

\subsection{Sunday, March 20, 2022}
No progress today. I was in Asheville with my family over the extended weekend.

\section{Week of March 21, 2022}
\subsection{Monday, March 21, 2022}
I created the first draft of my GANTT Chart today. Figure \ref{fig:gantt} contains the first draft of my GANTT chart. I also worked on my proposal for today. However, I can't seem to find a good acronym for the title of my project, which I want to be something along the lines of: Machine Learning for Text Simplification in Research Papers.

\begin{figure}
    \centering
    \includegraphics[scale=0.05]{images/RCompSci2022GANTTChart (1).png}
    \caption{First draft of GANTT chart}
    \label{fig:gantt}
\end{figure}

\subsection{Tuesday, March 22, 2022}
Today I wrote the intellectual merits part of my preliminary proposal and also created an outline for the rest of the proposal. I also wrote the overview, dissemination, and finished the project personnel part of my proposal.

\subsection{Wednesday, March 23, 2022}
\begin{itemize}
    \item Came up with the name of my project: Machine Learning for Research Paper Simplification (MAL-
REPS)
    \item Wrote the overview of my project
    \item Exapnded on RQ1, 2, and 3 and added them to the proposal.
    \item Rationale for MALREPS
    \item Theoretical framework (started on it)

\end{itemize}



\subsection{Thursday, March 24, 2022}
I FINISHED WRITING MY PRELIMINARY PROPOSAL TODAY AND I TURNED IT IN!!!

\subsection{Friday, March 25, 2022}
Today I watched the video \href{https://www.youtube.com/watch?v=7kLi8u2dJz0}{"What is BERT? | Deep Learning Tutorial 46 (Tensorflow, Keras & Python)"} which goes over what BERT is and how to use it.

How can similarity be captured between different words? You can look at features between different words.
\begin{enumerate}
    \item Look at various features of the input text
    \item Create vectors of the values of each feature of each input text (called a word embedding)
    \item The model figures out each feature on its own (we don't know how ML works)
    \item BERT can generate contextualized embedding (unlike something such as Word2Vec), which means that the same word used in two different contexts will generate different embedding
\end{enumerate}

\subsection{Saturday, March 25, 2022}
Today I read over the TensorFlow hub documentation for how to preprocess and use the BERT model itself to create text embeddings. There are also many other models that can be used for word embeddings, such as ulmfit, electra, gtr, and more.

I also found a pretty interesting research paper that describes when BERT was created. I will probably read this paper in the upcoming days. The paper is here \href{https://arxiv.org/abs/1810.04805}{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
}

The BERT model source code is \href{https://github.com/google-research/bert}{here on GitHub}.

\subsection{Sunday, March 27, 2022}
Today I wrote code that is able to preprocess text and generate word embeddings using BERT from the tensorflow hub website.

\section{Week of March 28, 2022}
\subsection{Monday, March 28, 2022}
Today I watched the video \href{https://www.youtube.com/watch?v=hOCDJyZ6quA}{Text Classification Using BERT & Tensorflow | Deep Learning Tutorial 47 (Tensorflow, Keras & Python)}. Notes from my video are below:

\begin{itemize}
    \item Downsampling, take the smallest number of rows based on each category. Ex: x has 100 samples while y has 300 samples. Take 100 rows of x and 100 rows of y to train the data.
    \item sklearn.model\_selection has a function called train\_test\_split that is able to split pandas data frame into training and test data
    \item The code from this video is on \href{https://github.com/codebasics/deep-learning-keras-tf-tutorial/blob/master/47_BERT_text_classification/BERT_email_classification-handle-imbalance.ipynb}{GitHub}
    \item sklearn.metrics.pairwise has a function called cosine\_similarity that can measure the similarity between two sentences that have been turned into a vector with BERT.
    \item Metrics in ML don't actually have an effect on the model train (that works on the loss function)
    \item you can only rely on accuracy when using a BALANCED dataset
    \item Confusion matrix shows correct and wrong predictions
\end{itemize}

I sent emails to:
\begin{itemize}
    \item Mr. Miller (Leveraging BERT)
    \item Dr. Srivastava (NLP UNC)
\end{itemize}

\subsection{Tuesday, March 29, 2022}
Word2Vec is a method of turning a sentence into a sequence of numbers, because comoputers can only understand numbers. I watched the video: \href{https://www.youtube.com/watch?v=hQwFeIupNP0}{What is Word2Vec? A Simple Explanation | Deep Learning Tutorial 41 (Tensorflow, Keras & Python)}

\begin{itemize}
    \item Embeddings are created via neural networks.
    \item ``Self-supervised", meaning that there isn't necessarily a correct output
    \item Words that have a similar meaning result in similar vectors.
\end{itemize}

The way word embeddings are created:
\begin{enumerate}
    \item Take a fake problem (such as fill in a missing word in sentence--BERT)
    \item Solve it using neural network
    \item Word embedding is created as a SIDE EFFECT 
\end{enumerate}

I also began reading the research paper: \href{https://arxiv.org/pdf/2109.06838.pdf}{Employing Proverbs in Context as a Benchmark for Abstract
Language Understanding}

\begin{itemize}
    \item Email sent to Dr. Bansal (mbansal@cs.unc.edu)
    \item Email sent to Dr. Dhingra (bdhingra@cs.duke.edu)
    \item Email sent to Dr. Barzilay (regina@csail.mit.edu)
\end{itemize}

\subsection{Wednesday, March 30, 2022}
Today I prepared the Python workshop that I will be presenting to the class on Thursday (3/31). The topics that I created code for are:
\begin{itemize}
    \item Basic Python syntax
    \item data science (tables, datasets, plotting, analyzing data)
    \item sentiment analysis
    \item Classifying movie reviews
\end{itemize}

\subsection{Thursday, March 31, 2022}
Today I presented the Python workshop in class. I think that it went really well, except for a few parts where Mr. Gotwals said that R was better than Python O-O

Emails sent today:
\begin{itemize}
    \item Dr. Chaturvedi (snigdha@cs.unc.edu)
    \item Dr. Woodsend (email failure)
\end{itemize}

\subsection{Friday, April 1, 2022}
In class today, we learned about file management, UNIX commands, and how computers really work. I think that taking out ``since you're an expert, I was wondering...." makes my emails sound more professional. I also don't know whether or not keeping the line ``I would like to talk to you about my own research questions and hear your thoughts on them" is a good way to ask when emailing potential mentors.

Emails sent today:
\begin{itemize}
    \item Dr. Wiseman (swiseman@duke.edu)
    \item Dr. Wills (lisa@cs.duke.edu)
\end{itemize}

\subsection{Saturday, April 2, 2022}
Emails sent to today:
\begin{itemize}
    \item Dr. Fahlman (sef@cs.cmu.edu)
    \item Dr. Singh (singh@ncsu)
\end{itemize}

\subsection{Sunday, April 3, 2022}
Emails sent to mentors today:
\begin{itemize}
    \item Dr. Berg (berg.tamara@gmail.com)
    \item Dr. Frederking (ref@cs.cmu.edu)
    \item 
\end{itemize}

\section{Week of April 4}
\subsection{Monday, April 4, 2022}
Today we went over things that we need to change for our preliminary proposals. I think for me, the main things will be the work plan and the background sections. Today I edited the parts that were easier to fix, such as contractions and such.

\subsection{Tuesday, April 5, 2022}
No progress today. I had a datathon, and our team got honorable mention (2nd place)

\subsection{Wednesday, April 6, 2022}
Emails sent: 
\begin{itemize}
    \item Dr. Oliva (joliva@cs.unc.edu)
    \item Ms. Bauer (lbauer6@cs.unc.edu)
\end{itemize}

I also finished all of the ``easy" edits on my NSF proposal today. I added graphics, citations, fixed typos, expanded contractions, etc. I also started giving more background for machine learning.

\subsection{Thursday, April 7, 2022}
Today I drafted more mentor emails and read research papers regarding them. I also responded to mentor emails. Most of the mentors are rejecting me, and I don't know if I will be able to find a mentor for SRIP.

\subsection{Friday (April 8, 2022) - Sunday (April 10, 2022)}
I was at the Technology Student Association (TSA) State conference for this weekend. I didn't get a chance to work on RCompSci materials during this time.

\section{Week of April 11, 2022}
\subsection{Monday, April 11, 2022}
Today I emailed more mentors and set up a meeting with Vaibhav (PhD student) from NCSU. Vaibhav's work doesn't directly deal with NLP, but I wonder if I might be able to look into something with him.

\subsection{Tuesdday, April 12, 2022}
Today I finshed all the changed I need from my preliminary proposal. I will be proofreading and trying to lower my hemingway score in the next few days.

\subsection{Wednesday, April 13, 2022}
Today I put my final proposal through hemingway and grammarly and fixed grammatical errors. I turned in my final proposal today. I also got on a call with Vaibhav from NCSU, and he wants to work with me. However, he is more interested in social media text simplification rather than research paper text simplification.

\subsection{Thursday, April 14, 2022}
Today I set up my GitHub repository where I will be creating an npm package that adds word explanations to sentences.


\subsection{Friday, April 15, 2022}
Today I got the first draft of my word substitutions working. The endpoint is /api/simplify and it adds simplified definitions to the ends of words.

\subsection{Saturday, April 16, 2022}
Today I thought about possible projects that I might want to complete with the 3rd year graduate student at NCSU. My ideas are below:

\begin{itemize}
    \item Doing text simplification on reddit posts to extract the general gist of information, this would allow more information to be spread on reddit, as people don't usually read super long posts.
    \item Doing summary creation on reddit posts to generate a summary of a single post
\end{itemize}

\subsection{Sunday, April 17, 2022}
No progress today, I was enjoying Spring break with my family.

\section{Week of April 18, 2022}
\subsection{Monday, April 18, 2022}
Today I fixed an error on the API endpoint where if you gave it an unkown word that wasn't in the dictionary, it would cause an error. Now, it no longer throws an error. I also created an NPM package for my project.

\subsection{Tuesday, April 19, 2022}
Today I watched the video: \href{https://www.youtube.com/watch?v=TQQlZhbC5ps}{Transformer Neural Networks - EXPLAINED! (Attention is all you need)}, which explained the paper Attention is all you need. This paper introduced the concept of transformers.

\begin{itemize}
    \item Vector sequence models produce a variable length output
    \item Sequence vector models produce a fixed length output
    \item Sequence to sequence models produce variable length output
    \item Normal recurrent neural networks are VERY slow to train. Each word is passed in one after the other (time)
    \item With transformers, the embeddings are created all at once (simultaneously).
    \item Attention vectors are created using transformers, which represents how important a word is
\end{itemize}

\subsection{Wednesday, April 20, 2022}
Today I watched the video \href{https://www.youtube.com/watch?v=XcZGKAF5zxg}{Text Summarization in SpaCy and Python}. They took the entire document and found the most common words that appeared the most number of times throughout, and then chose the sentences with the highest occurrences  of the words with the most occurrences. This method of extractive text summarization may work, but I think it would be worth it to compare this method of text summarization on research papers compared with the k-means clustering method and and seeing which performs better.

\subsection{Thursday, April 21, 2022}
Today I watched the video \href{https://www.youtube.com/watch?v=Yo5Hw8aV3vY}{Automatic Summarization using Deep Learning | Abstractive Summarization with Pegasus}. They used Pegasus to perform extractive text summarization on news articles and wikipedia articles but also SCIENTIFIC RESEARCH. The abstractive model here works really well, and I think that I will be able to use this model in my project as well. It is able to simplify an abstract of a research paper really easily. They also have a model that is specially trained with the PubMed dataset, which may create better abstractive text summarizations.

\subsection{Friday, April 22, 2022}
Today I installed gnuplot on my computer and signed up for a supercomputing account for our labs next week. 

\subsection{Saturday, April 23, 2022}
No progress today, I was at a competition all day.

\subsection{Sunday, April 24, 2022}
Today I prepared for my meeting with Vaibhav, who said that he wants to do research with me related to social media and hot topics (asian hate crimes, LGBTQ+ rights, racism, the Black Lives Matter movement, etc). The topic that I have the most hope for in my work with Vaibhav is doing text simplification on reddit posts about Asian hate crimes.

\section{Week of April 25, 2022}
\subsection{Monday, April 25, 2022}
Today I got with Vaibhav and Mr. Gotwals together on a call. This call went super well, Vaibhav has agreed to formally mentor me over the summer. Additionally, the lab PI, Dr. Singh, is allowing me to go to NC State over the summer each day to work in their lab. I think the most important step for me to now do is finding subreddits that we might be able to do text simplification on and contain relevant stories about asian hate crimes.

\subsection{Tuesday, April 26, 2022}
Today I created a list of subreddits that I thought we might be able to find posts about personal experiences relating to asian racism and hate from. However, many of the Reddit posts that I found that were in these subreddits were either news stories or just contained a link to a different news article. This made it very difficult to actually perfom text simplification, unless we went to the link contained within the post and scraped the information from there. Looking at ICWSM papers though, I think 

\begin{itemize}
    \item It's a Thin Line Between Love and Hate: Using the Echo in Modeling Dynamics of Racist Online Communities (for dataset of racism)
    \item Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model (for subject extraction)
\end{itemize}

\subsection{Wednesday, April 27, 2022}
Today I read over Gotwals guide for supercomputing. Today we also went over the introductory slides on the Bridges 2 supercomputer.

\subsection{Thursday, April 28, 2021}
Today I was able to debug the code I had with pegasus for abstractive text summarization. I think that we should also be able to use GPTJ for this summarization as well. My next step will be figuring out how to deploy this model onto an endpoint so any API call can make the request.

\subsection{Friday, April 29, 2022}
Today we did gnuplot in class. I also created a ML model using the huggingface API and Allen NLP that is able to take in a long piece of text, a question, and the number of bullet points that the user wants to get back, and we give them a summary. I deployed this on gradio, and comes with an API endpoint.

\subsection{Saturday, April 30, 2022}
Today I created a frontend for the API endpoint that I created yesterday. https://verste-ovz2hei6k-ganning127.vercel.app/simplify

\subsection{Sunday, May 1, 2022}
Today I watched another \href{https://www.youtube.com/watch?v=4Bdc55j80l8}{video} explaining the paper ``Attention is all you need", which was the video that invented transformers, a type of neural network.
\begin{itemize}
    \item encoder - maps input sequence to a continuous representation of the input, which holds all the info from previous inputs
    \item decoder - generates output while keeping in mind all the previous outputs
\end{itemize}

The steps that transformers use:
\begin{enumerate}
    \item Input embedding - create vectors (numerical representations of the sentence). An example is shown in ``embedding.png".
    \item Multi-headed attention: how words are associated with each other (if a single word occurs more often with another word)
\end{enumerate}

Convolutional neural networks are typically used for images, as they scan a window of frames. Before transformers, recurrent neural networks were used (looked at each word at a single time). They process words in order, so start from word 1 and end at final word. This had several issues: by the time you got to the end of the paragraph, the network already forgot what was at the beginning, can't do parallel processing. 

Transformers are much better, they can keep attention, be trained in parallel, are are much faster to train. Transformers look at positional encodings and SELF-attention. Positional embeddings meant that each word had its own vector based on its position in the sentence, so the position meaning was built into the word itself. While attention has been around for a long time, the new item about this paper was SELF attention.

\section{Week of May 2, 2022}
\subsection{Monday, May 2, 2022}
Today we went over fundamentals of machine learning, including supervised and unsupervised learning, cluster analysis, PCA, dimensionality reduction, and more. We also went through three mathemtica notebooks on machine learning. We talked about how various models could be used to classify artwork to an author, show the network of relationships between trump and russia, and show how we can attribute a work of writing to an author.

\subsection{Tuesday, May 3, 2022}
Today I was looking for datasets related to racism. I found a CSV file of 56k tweets from Donald Trump about various topics that he has talked about. Also, I found a paper titled \href{https://ojs.aaai.org/index.php/ICWSM/article/view/18050/17853}{X-Posts Explained: Analyzing and Predicting Controversial Contributions in Thematically Diverse Reddit Forums} that has a good looking reddit dataset, but I cannot access it. I emailed the researcher there, in hopes of getting the dataset. 

\subsection{Wednesday, May 4, 2022}
Today I watched the video: \href{https://www.youtube.com/watch?v=jgKj-7v2UYU}{Easy Custom NLP T5 Model Training Tutorial - Abstractive Summarization Demo with SimpleT5}, which is a open source python package (Simple T5) that can be custom trained for your own dataset. I think for my project, I could use Simple T5 to train my own model for abstractive text summarization using various models from huggingface. 

\subsection{Thursday, May 5, 2022}
I am struggling to find research papers that relate to my topic of subject extraction from racist reddit posts. Thus, I also cannot find many datasets with this purpose either. Some other possible ways to go with this might be fake news detection. 

Today in class we did cluster analysis with machine learning.

\subsection{Friday, May 6, 2022}
Today in class we went over clustering techniques using R. This is how k-means clustering works:
\begin{enumerate}
    \item Plot all data points on an appropriate representation (graph, number line, etc)
    \item Put k number of cluster points directly on the representation just chosen (the best number for k can be chosen through the elbow technique)
    \item Assign each point to the nearest cluster based on euclidean distance. 
    \item Move each cluster to the middle of all of the data points assigned to it
    \item Repeat until the cluster points do not move anymore
\end{enumerate}

I also met with my mentor today. We are probably not going to go with the racism idea because there are not enough data sets out there that I was able to find that related to racism. For the next week, before we meet, we will each be looking over various papers and find a paper where the topic interests us.

\subsection{Saturday, May 7, 2922}
No progress today. I am studying for my AP Calculus BC exam.

\subsection{Sunday, May 8, 2922}
No progress today. I am studying for my AP Calculus BC exam.

\section{Week of May 9, 2022}
\subsection{Monday, May 9, 2022}
Today I began working on the summer report presentation that we will be giving to the rest of the group. I am a little unsure about what I will be doing in the summer still, research wise, because we are still deciding on and finalizing our idea. However, it will likely be something to do with machine learning and text simplification on social media posts. Vaibhav is more into social media posts and I am more into text simplification, so we will meet in the middle.

\subsection{Tuesday, May 10, 2022}
Today I kept working on the summer report presentation that I will be giving to the group. I am still stuck on trying to figure out a topic that we will work on, but I think that I will be reading various research papers from ASONAM, WEbScie, The web conference conferences because they are related to social media. To find accepted papers for each conference, I can search NAME + ``Accepted Papers"

\subsection{Wednesday, May 11, 2022}
Today I added some sections to my summer report (api endoint, and already published website) and finished up and prepared planning for what I am going to say for these Summer Reports.

\subsection{Thursday, May 12, 2922}
Today I read over papers from the ASONAM conference. I can't find many promising papers yet about topics that I'm interested in though. It was also very difficult to find papers from the ASONAM conference.

\subsection{Friday, May 13, 2022}
Today I looked at papers from the WEBSCI conference, which has papers that seem more promising than the one I looked at yesterday. Here are a couple of papers that I might explore further.

\begin{itemize}
    \item \href{https://arxiv.org/abs/2102.03870}{"Short is the Road that Leads from Fear to Hate": Fear Speech in Indian WhatsApp Groups}. Coded in Python, their code is at \href{https://github.com/hate-alert/Fear-speech-analysis}{github.com/hate-alert/Fear-speech-analysis}
    \item \href{https://arxiv.org/abs/2004.04046}{“Go eat a bat, Chang!”: On the Emergence of Sinophobic Behavior on Web Communities in the Face of COVID-19}
     \item \href{https://arxiv.org/pdf/2011.00449.pdf}{Improving Cyberbully Detection with User Interaction}
\end{itemize}

\subsection{Saturday, May 14, 2022}
Today I read the paper, “Short is the Road that Leads from Fear to Hate”: Fear Speech in Indian WhatsApp Groups. In text preprocessing, they only kept tweets that were in English and Hindi, covering over 70\% of the data. To filter out spam messages, a high precision lexicon set was used to remove spam. Then, they removed emojis, stop words, and URLs using regex. The multilingual tokenizer CLTK was used to tokenize sentences. 

The purpose of the paper was to identify fear speech messages against Muslims on whatsapp.

Annotations were made by humans to decide whether or not a post actually depicted hate speech.

Docanno can be used for humans to annotate texts

\textbf{Results}
\begin{itemize}
    \item The lifespan of a fear speech message was longer than that of a non fear speech message.
    \item If we are going to ask people from outside NCSSM/NC State to help us with our research, the response rate will likely be very low.
\end{itemize}

\subsection{Sunday, May 15, 2022}
The paper, “Go eat a bat, Chang!”: On the Emergence of Sinophobic Behavior on
Web Communities in the Face of COVID-19 is about confirming that anti-asian behavior and hate speech actually did increase due to COVID-19.

Datasets were collected from Twitter using the Streaming API and 4chan.

\textbf{Ideas}
I'm thinking that for my RCompSci project, we might be able to do something related to confirimg the prescence of something related to Covid-19? Maybe not directly related to text simplification anymore.

Today I also read the paper "Improving Cyberbullying Detection with User Interaction", which aims to model the behavior of those who cyberbully others through MULTIPLE comments posted, not just one.

We could do asian american racism detection. Comparing the reach of posts and such that are racist towards Asian Americans compared to all other posts on a platform such as Twitter.


\section{Week of May 16, 2022}
\subsection{Monday, May 16, 2022}
Today I met with Vaibhav about our RCompSci project. He was busy last week so this week we will all read papers and talk about them on friday to discuss possbile project ideas that we might want to pursue together.

I also read the paper \href{https://dl.acm.org/doi/pdf/10.1145/3485447.3512128}{Early Identification of Depression Severity Levels on Reddit
Using Ordinal Classification}, which is about classifying depression levels into 4 levels, instead of a binary classification of yes/no.

This was their model architechture:
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/id_depress_arch.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\subsection{Tuesday, May 17, 2022}
Today I read the paper \href{https://arxiv.org/pdf/1703.04009v1.pdf}{Automated Hate Speech Detection and the Problem of Offensive Language∗}, which attempted to classify hate speech from Twitter. Their model accuracy was around 91\%, but when they tested the model on testing data, it only had an accuracy of around 41\%, which may suggest overfitting. A few key takeaways:

\begin{itemize}
    \item Logistic regression was used to do dimensionality reduction on the data
    \item scikit-learn and SVMs were used to classify tweets ad hate speech/not hate speech.
\end{itemize}

Today I also looked at the code from "Deep Learning Models for Multilingual Hate Speech Detection". They have their dataset on \href{https://github.com/hate-alert/DE-LIMIT}{github.com/hate-alert/DE-LIMIT} as well!

\subsection{Wednesday, May 18, 2022}
Today I signed up for a research gate account. In class we went over how to do summer research and I think that one of my weaknesses is definitely needing to speak up more, instead of always being shy.

Reddit posts can be extracted using the \href{https://praw.readthedocs.io/en/stable/}{PRAW API}.

Today I also read the paper ``Identification of Disease or Symptom terms in Reddit to Improve Health Mention Classification", which is about trying to identify posts where users are discussing health conditions rather than using disease and symptom terms for other conditions. This is known as Health mention classification (HMC).

An F-1 score tests a binary model's accuracy on a dataset. The model in this paper only had an F-1 score of 75\%.

\subsection{Thursday, May 19, 2022}
Today I met with Vaibhav again. We both agreed that going for the idea of extracting the specific sentences from fear speech containing messages would be a good idea. 

We need to try and look for patterns in the fear speech messages. For the message mentioned in the paper the order of fear speech is as follows:
\begin{enumerate}
    \item Citing something from history (using numbers or dates)
    \item Spreading fear through any means (misinformation, scaring others, etc). THIS IS THE SENTENCE THAT WE WANT TO EXTRACT.
    \item Asking others to share the message
\end{enumerate}

I have emailed two of the authors of this paper asking for access to their full dataset, as the one currently mentioned in the paper only contains about 8k tweets out of the full 27.

ALSO TODAY WAS THE LAST DAY OF CLASSSESSSS WOOOO!!! 

I think that tomorrow I will be looking at \href{https://raw.githubusercontent.com/hate-alert/Fear-speech-analysis/master/Data/fear_speech_data.json}{more of the fear speech JSON} file and trying to find more datasets.

\subsection{Friday, May 20, 2022}
Today I tried to find more datasets online about fear speech. While I didn't find many datasets that related to fear speech, I did find some that were more targeted towards hate speech, such as the \href{https://huggingface.co/datasets/hate_speech_offensive}{hugging face dataset}.

\subsection{Saturday, May 21, 2022}
When looking at the fear speech dataset that I mentioned on Friday, there doesn't really seem to be a pattern in which the fear speech sentence itself is happening in. However:
\begin{itemize}
    \item Many of the fear speech sentences appear with numbers in them (Example: "Jamaat-e-Islami has more resources than Pakistan, 70+ bank accounts, 350 mosques, 300 madrasas and 4500 crores Islam = terrorism")
    \item Many fear speech sentences also have emojis around the sentence (Example: "[EMOJI]  * * It is a district with a population of about 8 lakhs. Muslims * constitute * 70\% of the population here. This is the same district from where 3 Muslims went to * Syria * to be admitted to * ISIS *."), ("The cure for terrorism is not to kill the terrorist ✊  but to close his factory."),
\end{itemize}

\subsection{Sunday, May 22, 2022}
Toady I wrote a short Python script that turns the JSON file from the github into a more readable CSV file format. Also, there doesn't look to be too much patterns regarding the sentences right before / after the hate speech sentences specifically, but many of them do cite numbers / statistics and use emojis


\subsection{Monday, May 23, 2022}
Today I wrote more of the script and added sections that performed sentiment analysis on the text. Here are my observations:
\begin{itemize}
    \item Fear speech generally has more shares than regular posts
    \item Average length of a fear speech post is longer
    \item Fear speech has a lower average sentiment and is much more radical than normal speech.
\end{itemize}

Patterns in fear speech sentences:
\begin{itemize}
    \item First couple of sentences set the premise for the rest of the information
    \item Next couple sentence is provoking you and trying to get you to hate a certain community. Sentences that provoke directly target another community.
    \item Last the couple of sentences call to action or addressses the reader directly. Conclusion sentences are usually indirect but are implying something.
\end{itemize}

Process to going going about doing this
\begin{enumerate}
    \item manually annotate each sentence to separate the parts of (premise, provoking, call to action)
    \item train the model on input features of this sentence. the target for our model will be our annotation.
    \item Can we build a model that predicts the conclusion based on the provoking sentences?
\end{enumerate}

TODO: Read 10-15 more stories and discuss whether or not the sentence types that we discussed are present.

\subsection{Tuesday, May 24, 2022}
No progress today. I had my calculus final exam today.

\subsection{Wednesday, May 25, 2022}
Today I annotated 4 stories. They were mostly following the structure that we already had, which is really good. However, some of them I don't really understand, as they follow India's politics.

\subsection{Thursday, May 26, 2022}
Today I annotated 4 more stories. Today was also move out day so I didn't get much done.

\subsection{Friday, May 27, 2022}
Today I will be meeting with Vaibhav. Before the meeting, I read over some more posts and stories from the fear speech dataset. I finished all the stories that I was supposed to annotate. Here are some questions I did have, howver:

\begin{enumerate}
    \item Some of the ``sentences" don't end with periods, as the grammar in them is incomplete. How should we deal with this?
    \item What are we going to do about posts that have sentences that don't fall into a specific category of sentences that we have?
    \item How are we going to annotate all of these in such a way that a ML model is able to read through them and train itself?
\end{enumerate}

We need to come up with an actual problem statement that gives the ``motivation" behind why we are doing this research, which would allow us to publish this research. We need to be very clear about what work has previously been done. Then we can find gaps and find a problem statement in which we are able to publish papers.

TODO: Also, read more papers for hate speech.

\subsection{Saturday, May 28, 2022}
No progress today. I had a competition all day and it is summer break.

\subsection{Sunday, May 29, 2022}
Today I found some papers that related to hate speech and fear speech. I think that I will be reading the following papers in the days to come:

\begin{itemize}
    \item \href{https://sci-hub.hkvisa.net/10.1145/3232676}{A Survey on Automatic Detection of Hate Speech in Text}
    \item \href{https://aclanthology.org/W12-2103.pdf}{Detecting Hate Speech on the World Wide Web}
    \item \href{https://arxiv.org/abs/1712.06427}{Detecting Hate Speech in Social Media}
    \item \href{https://sci-hub.hkvisa.net/10.1109/ATSIP.2018.8364512}{On the use of pitch-based features for fear emotion detection from speech}
    \item \href{https://www.isca-speech.org/archive_open/sp2004/sp04_205.pdf}{F0 and pause features analysis for Anger and Fear detection in real-life spoken dialogs}
\end{itemize}

\section{Week of May 30, 2022}
\subsection{Monday, May 30, 2022}
Today I am going to read over the papers that I said I would read over yesterday. The first paper, \textbf{Survey on Automatic Detection of Hate Speech in Text} points out that there is a lack of data about hate speech, automatic techniques for hate speech detection is not available, creates a definition for hate speech:

\begin{itemize}
    \item Hate speech has specific targets (ethnic origin, religion, other)
    \item Hate speech is to incite violence or hate amongst a group
    \item Hate speech is used to attack or diminish another group
\end{itemize}

The most common algorithms used to detect hate speech are SVM, random forests, and decision trees. This paper just provided a background on the work that was previously done.

The paper \textbf{Detecting Hate Speech on the World Wide Web} focuses on creating a method to automatically classify hate speech from regular text with 94\% precision. They noted that:

\begin{itemize}
    \item hate speech uses a small set of high frequency stereotypical words
    \item they used an SVM to classify hate speech from non hate speech.
\end{itemize}

The paper \textbf{On the use of pitch-based features for fear emotion detection from speech} and \textbf{F0 and pause features analysis for Anger and Fear detection in real-life spoken dialogs} focuses on detecting emotion through a person's spoken word, which isn't really about machine learning lol.

\subsection{Tuesday, May 31, 2022}
The paper, \textbf{A Survey on Hate Speech Detection using Natural Language Processing} defines hate speech as something that \emph{ any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic}. Below are some methods usually used in hate speech detection:
\begin{itemize}
    \item Bag of words: Supervised classification technique. Very simple, bu requires that the same predictive words that appear in the training data must appear again in the testing data.
    \item Brown clustering has been used to assign individual words to produce hard clusters. 
    \item Sentiment analysis is sometimes used, as a negative sentiment is said to usually be a hate speech message.
    \item Bayesian Logistic Regression
    \item Hate speech must be read in context, as some sentences by themselves might not be hate speech, but when taking into context the site / environment in which they were posted, they might be hate speech then.
    \item When classifying hate speech from non-hate speech, SVMs are mainly used.
    \item Data for hate speech detection is usually collected from Twitter, Yahoo!, Formspring, Whisper, and Xanga.
\end{itemize}

\subsection{Wednesday, June 1, 2022}
The original paper we read, \textbf{Short is the road that leads from fear to hate: fear speech in indian whatsapp groups} had several results regarding basic analysis between fear speech and regular speech: 
\begin{itemize}
    \item Fear speech typically lasts longer (is continously seen by more people) than regular speech
    \item Fear speech generally has more shares per post than regular texts.
    \item Emojis were more often used in fear speech than regular texts.
    \item They got around an MAX 83\% accuracy. Could we be able to build on this?
\end{itemize}

\textbf{Possible problem statement:} Infering what the people are trying to say from provoking sentences. Vaibhav says that each type of provoking sentences tries of convey a different type of fear.




1. how does this beenefit anyone for publication - benefits people by making things more black and white. also add what type of fear they are inducing. Based on each sentence in a post, we can predict multiple inferences 

for example, if the sentence relates to previous rulers, it might be trying to say that they were atrocious

2. are we generating the conclusion or are we extracting it? 
    a. do drawing conclusions for every single sentences
    b. first do data annotation
    
Processing:
a. phase where me and vaibhav go through and see the red sentences
b. do data annotation and break the dataset into multiple sentences. classify 
c. come up with around 3 category labels for each sentence.

Next Steps:
1. create a new sheet with shuffled rows (do not modify original)
2. do top 20 fear speech and mark sentences with their categories (they need to be red)
3. do around 3 labels
4. do not look at Vaibhav's sheet

\subsection{Thursday, June 2, 2022}
Today I came up with the categories that I will be looking for in the provoking sentences. They are:
\begin{itemize}
    \item Fear about politics (stuff about political leaders, elections)
    \item Fear about religion (items about how a certain religion is bad or that they will do something)
    \item Fear about terrorism or violence  (items about conflicts, deaths)
\end{itemize}

Each provoking sentence may take on multiple labels. 

\subsection{Friday, June 3, 2022}
Today I annotated 3 posts from the fear speech dataset. Most of the posts I've realized that only contains sentences from the same category.

\subsection{Saturday, June 4, 2022}
I just realized that I have been annotating posts from normal speech instead of fear speech. That is quite annoying lol. However, I did notice something.

\begin{itemize}
    \item Normal speech posts usually have one type of provoking sentence, whether that be politics, religion, or terrorism/violence.
    \item Fear speech posts usually have around 1-2, types of provoking sentences
\end{itemize}

\subsection{Sunday, June 5, 2022}
I annotated around 8 more posts today. Tomorrow I will be reading about more papers relating to fear speech and hate speech.

\subsection{Monday, June 6, 20222}
Today I read around 3 more fear speech (long) posts from the dataset that we collected. Most of these posts were very confusing for me to understand, as many had references to Indian figures who I do not know.

\subsection{Tuesday, June  7, 2022}
Today I read over the code of the paper ``HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection", which created a new dataset that created a dataset with:
\begin{itemize}
    \item Label: whether or not the piece of text is hate speech
    \item Targets: the target community of the piece of text (i.e. African Americans)
    \item Text: the piece of text itself. This piece of text also contains an annotated portion that was the que for the annotator to choose hate speech / normal / offensive speech.
\end{itemize}

The dataset they used is \href{https://github.com/hate-alert/HateXplain/blob/master/Data/dataset.json}{on github}.

I also learned that the differenece between hate speech and offensive speech was that hate speech is more threatening and includes a call to violence / action, when offensive speech degrades another group.

\subsection{Wednesday, June 8, 2022}
Today I read over the actual paper ``HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection". The introduction section just details previous work that has been done and how that fails to meet the model the paper created's standards. The introduction also gives a brief overview of their methods and what they did.

To evaluate their code, they used the F-1 score, accuracy, macro F1-score, and AUROC score.

\subsection{Thursday, June 9, 2022}
Today I will meet with Vaibhav to discuss our progress so far.


\begin{itemize}
    \item \textbf{Possible problem statement}: Sentences written about fear speech are indirect, and that's why we need to make a system to identify the real intentions behind writing that sentence.
    \item There already has been a paper that identifies fear speech messages
    \item However, they haven't specified what type of fear (or provoking) is being shown 
    \item What the paper found fearful seems provoking to us, rather than fearful.
    \item Just improving the accuracy might not be enough.
    \item \textbf{Our contributions}: our annotated dataset, model (training and such)
\end{itemize}

In our cause, if we label the sentences for types of provoking, this model can be deployed on an online platform and this allows companies to mark those sentences as the type of provoking.

TODO:
\begin{itemize}
    \item Have a more thought out motivation (who will be helped, why are we doing this)
    \item Split up each post into individual sentences and then we begin annotating (only fear speech) and populate into new google sheet
    \item Find more data on fear speech (maybe scrape some from twitter with keywords such as ``terrorist")
    \item Find conferences that we can submit to in Sep / Oct (\href{https://www.junglelightspeed.com/the-top-10-nlp-conferences/}{here}
\end{itemize}

\subsection{Friday, June 10, 2022}
Today I wrote out the motivation for our research tasks on \href{https://docs.google.com/document/d/17kLWPe7UvaFoTWG4jR4-Zsl5Ngrt6Gf5Pqt5q5fmRsw/edit}{this google doc}.

\subsection{Saturday, June 11, 2022}
Today I wrote a short Python script that will turn the data we originally had for the dataframe into individual sentences.

The Python script is on \href{https://colab.research.google.com/drive/1SiPwRhosjSFBRKsibU_2UrM60fwUtp9I?usp=sharing}{Google Colab}. However, some of the sentences on here are just single letters / numbers. They're not actually a full sentence.

\subsection{Sunday, June 12, 2022}
Today I created a list of conferences that I think we should be able to submit to. The Google sheet is linked \href{https://docs.google.com/spreadsheets/d/1xaAznWx4lQz2tLoyNnGAnjFkoCbv78nOoP6YjoFKf5Y/edit#gid=0}{here}.

\section{Week of June 13, 2022}
\subsection{Monday, June 13, 2022}
Today was the first day of SRIP. We went over mentor expectations and orientation material for this program. I also filled out the planner for tomorrow on what I will be working on in the lab.

\subsection{Tuesday, June 14, 2022}
Today I went in person to NCSU for the first time. I mainly worked in the library building (Hunt library) and I further developed the motivation for our research project. I also revamped the script that we had earlier to use nltk's sentence tokenizer instead of just splitting on periods. I worked on a presentation script today because I will be presenting the status of our research project to the grad student's supervisor tomorrow.

Link to \href{https://docs.google.com/document/d/1p_nFBoggFyVAgsPvoWxGu3Vn5lqnA7eN9yLNTDxCdVI/edit?usp=sharing}{meeting notes}

\subsection{Wednesday, June 15, 2022}
Today I went to the NCSU lab again with my mentor. I presented my research to the rest of the research group as a check-in. They suggseted that we look into posts not just from Indian WhatsApp groups, but expand into the US by looking at topics such as gun violence. However, it would be too diffitcult to expand our categories to different topics to include gun violence. Both Vaibhav and I agree on not including this data. Things I've done today:

\begin{itemize}
    \item Random sample of 10k sentences from the dataframe
\end{itemize}

Link to \href{https://docs.google.com/document/d/1xcxDhuWBfwrEzA_Y6ZK-tRo33wt-DthB-97ZrMezN8w/edit?usp=sharing}{meeting notes}

\subsection{Thursday, June 16, 2022}
Today I annotated 200 sentences from the individual sentences dataset. After annotation in the morning, I met with Vaibhav in the afternoon to go over annotation and discussions to go over our disagreements. It took us 1.5 hours to go over just 30 posts, which is an average of around 3 minutes / post.

Additionally, I wrote a short python script for fun that classifies provoking sentences from non-provoking sentences. This model achieved a maximum of 68\% testing accuracy.

I also calculated the cohen-kappa score from our differing annotations and got 0.26. This is really low. Tomorrow, after our discussion today, we will be annotating 200 more posts to see if our cohen kappa score inreases.

Link to \href{https://docs.google.com/document/d/1ZV9jps3gLQnBNUDG9ZCDrG-zob538XkiNL1clB8kYK8/edit?usp=sharing}{meeting notes}

\subsection{Friday, June 17, 2022}
Today I met with Vaibhav in the morning to go over the sentences that we annotated. We came up with the rule that everything that is not ``none" MUST be related to religion somehow, as Vaibhav told me that that is how you get accepted into the most conferences. 

I annotated 200 more sentences today, came up with the annotation guide, and also wrote a script that takes all the raw WhatsApp posts and splits them into individual sentences (ALONG with the sentence directly before and after the target sentence). This allows us to provide context when doing annotations. Vaibhav said that there is a method to feed multiple sentences into a machine learning model at once, which is why we are able to do this.

Link to \href{https://docs.google.com/document/d/1oBA7dLgPfAtDW8P7GTJky1KsGvf8_E19y6jDpj0e1SA/edit?usp=sharing}{meeting notes}

\subsection{Saturday - Sunday, June 18-29, 2022}
No progress these days because I had a hackathon.

\section{Week of June 20, 2022}
\subsection{Monday, June 20, 2022}
Today I annotated around 50 nmore fear speech posts, as well as read over about multi-label classification. The python package simpletransformers has an example of multi-label clssification that I might be able to use on this dataset. I will create a script that attempts to do this tomorrow. I also went to the mentorship break event today and won a ping pong plushie from the tournament.

\subsection{Tuesday, June 21, 2022}
Today I annotated 100 posts for the individual sentences dataset. I also calculated the cohen kappa score in Python, along with the krippendorf alpha score. I will be calculating our fliess kappa score soon.

Our fleiss kappa score was 0.57, which is already much better than the original dataset's fleiss kappa score of around 0.37.

I also annotated 100 more posts this night.

Link to \href{https://docs.google.com/document/d/1RadyoYOYbrsICxQLE5TTWOezY15mMqB6Yd6E3F3n8PI/edit?usp=sharing}{meeting notes}.


\subsection{Wednesday, June 22, 2022}
Today I looked into the differences between Cohen Kappa annotation scores and Krippendorf alpha scores. I think that for our purposes, Cohen Kappa scores will be better as we area closer to being satisfcatory for that inter-rater agreement than for the krippendorf alpha one.

Additionally, I wrote two scripts that used an SVM and Naive Bayes algorithm to detect for fear speech and got around 84.3\% accuracy. This is higher than the paper got. Additionally, I wrote a method to test this accuracy with any piece of text the user enters.

I also met with my mentor today and we decided to use Cohen Kappa score for inter-rater reliability. Our Cohen Kappa score for some reason is not increasing from yesterday, as we had exxactly 0.53 for both days. I also revised my 300-400 annotations, read over various disagreements, and will be annotating 100 more posts tonight.


\subsection{Thursday, June 23, 2022}
Our annotations from yesterday saw a Cohen Kappa score of 0.53, which was lower than the previous days' 0.59. I met with Vaibahv today to go over our disagreements on the posts, and below are a few important points from our discussions:
\begin{itemize}
    \item If a post is unclear or doesn't have enough information, mark it as none
    \item If a post is not targeted toward a specific religious group (such as politics or general violence), mark it as none
    \item If the before and current sentences are related, then we are able to say that the provocation in the previous sentence is also in the current sentence, as readers will read previous sentence before current sentence in the real world.
\end{itemize}

Today I will be annotating 100 more posts, revising my reviews from 400-450, and reading over the disagreements of annotations so we are on the same page. I will be reading the labels that Vaibhav's were correct and Vaibhav will be reading the labels that I said were correct.

Link to \href{https://docs.google.com/document/d/1Bzm7CkOvRsMDzUCUNdFKpT2kQTxcEdSJR-o43tM2Jsg/edit?usp=sharing}{meeting notes}


\subsection{Friday, June 24, 2022}
Today I was playing around with the script that does provocative sentence classification again and got around a 68\% accuracy. The original fear speech detection is 84\% accurate. I wrote the overview of my project today, including the contributions that our project will make, a very drafty abstract and introduction, and a timeline of when each step in our project should be done by.

I will annotate 200 more sentences later today and meet with my mentor. Last night, Vaibhav and I's annotation reached a cohen kappa score of around 0.73, which is really good.


I also figured out a way to translate from English --> ASL Gloss. While the model isn't perfect yet, it achieved a BLEU score on average of about 51, which is interpreted as ``Very high quality, adequate, and fluent translations". I'm thinking about making this my second RCompSci project.

\subsection{Saturday, June 25, 2022}
Today I went through the disagreements that Vaibhav and I had from the disagreements from 400-500. 

\subsection{Sunday, June 26, 2022}
Today I annotated 100 more posts from 500-600.

\section{Week of June 28, 2022}
\subsection{Monday, June 27, 2022}
Today I created an API endpoint in Python and deployed it on Heroku that was able to use SVMs to detect for fear speech.

Additionally, I used BERT and keras layers to create a model to attempt multi-class classification.

I also completed my presentation draft (abstract and presentation) and will be reviewing it before I actually record it.

Link to \href{https://docs.google.com/document/d/1yPdZDb-eeijU4GWeDnLeWesqKLKzm0n0pdUh024UHcc/edit?usp=sharing}{meeting notes}

\subsection{Tuesday, June 28, 2022}
Today I reviewed the sentences where we disagreed on the 100 that we discussed yesterday (500-600). I also read over the pixie paper and the concept of using transformers to adapt them to our methods for our paper.
However, the wifi went out yesterday as well, so I wasn't able to work on annotating sentences.


The cohen kappa score for today's annotations was 0.7, which is really good.

I finished filming my presentation and submitted it to NCSSM's SRIP office today!

\subsection{Wednesday, June 29, 2022}
These are the items I worked on today
\begin{itemize}
    \item I read over our disagreements from annotations 600-700 and added the final labels.
    \item Finished annotations for sentences 700-800
    \item Discussed with mentor our about disagreements and calcuated a cohen kappa score of 0.63
    \item Setup our docanno annotation platform for 6000 posts
    \item Read over methods that were previously used for multi-class text classification in Python.
    \item Annotated 100 posts.
\end{itemize}

\subsection{Thursday, June 30, 2022}
Today I:
\begin{itemize}
    \item Created my final RCompSci presentation for SRIP
    \item Practiced my final SRIP presentation
    \item Met w/mentor and discussed on how to label fear speech when it comes up in our annotations
    \item Split up dataset for annotation of fearful vs. provoking
    \item Set up the mturk account and annotation system (however, it only accepts regular text and not emojis, so I'm not sure on what to do with that.
\end{itemize}

Link to \href{https://docs.google.com/document/d/1kZTLp8gpHfW_Fc1MB4RqrThNCKRpGbhtCXggADEOW5w/edit?usp=sharing}{meeting notes}

\subsection{Friday, July 1, 2022}
Today was the final day of SRIP. I did my RCompSci presentation today in the lecture hall, along with the rest of the RCompSci folks.

\subsection{Saturday, July 2, 2022}
Today I tried to set up AWS MTurk requester sandbox, but it kept giving me errors:

\begin{enumerate}
    \item Each batch can only be 500 rows
    \item Why did it say that it was going to cost me like \$5?
\end{enumerate}

\subsection{Saturday - Sunday, Jul 2 - 3, 2022}
I was very sick these days so I wasn't able to get much done.

\section{Week of July 4, 2022}
\subsection{Monday, July 4, 2022}
Today I annotated sentences to be fearful vs. non-fearful. I only annotated sentences where the final label was "none" or "undecided", as we are only looking for posts where the sentence is only fearful and not provoking. If the label was anything else, then it was provoking.

\subsection{Tuesday, July 5, 2022}
Today I met with Vaibhav and we went over next steps that we will need to do for our project. We need to annotate sentences that we have marked as oppressive and determine whether or not they are actually oppressive or just fearful. Our points are below:

\begin{itemize}
    \item Only look at fearful vs. not-fearful at only sentences that were marked as oppression.
    \item Send Vaibhav message when I'm done
    \item Annotate 100 posts
\end{itemize}

Link to \href{https://docs.google.com/document/d/1eEZaqT8RIj2LU541y_P7s_mW0gKrRpY3Ivmap_tp094/edit?usp=sharing}{meeting notes}.

\subsection{Wednesday, July 6, 2022}
Today I went through and annotated my share of the posts as fearful vs. non-fearful in all posts with "oppression" or "Undecided" as the final label.

\subsection{Thursday, July 7, 2022}
Today I went through the 10k posts with before and afters and annotated 100 posts. I am finished with phase 1.

\subsection{Friday, July 8, 2022}
No progress today. I am waiting on my mentor to finish his annotations.

\subsection{Saturday, July 9, 2022}
I was completely sick today, so no progress.

\subsection{Sunday, July 10, 2022}
I am meeting with Vaibhav at 9PM to begin our phase 3 annotations. Here are a couple of tools that I've tried in the past for phase 3:

\begin{itemize}
    \item Docanno: clean and visually pleasing UI. however, i've had all anotations completely been wiped twice already because of the app crashing, resulting in a complete loss of data. I don't think this is the most reliable route to go.
    \item Amazon Requester Sandbox: i think the maximum batch size was only 500 annotations? AWS Says: ``Too many input rows. The maximum number of tasks per batch is 500. For more information, please refer to our FAQ."
\end{itemize}

\section{Week of July 11, 2022}

\subsection{Monday, July 11, 2022}
Today I met with Vaibhav and we discussed how we are going to go about phase 3. However, Amazon MTurk wasn't working on the call, so Vaibhav will talk to a few of the people he knows about it and will get back to me in the next few days on how to make MTurk work. 

\subsection{Tuesday, July 12, 2022}
I tried what Vaibhav said about creating an MTurk worker account. I really don't think it is worth the wait to wait 3 days before we can start our annotations. I think I will be starting m phase 3 tomorrow so we don't fall behind.

\subsection{Wednesday, July 13, 2022}
Today I wrote the script that created files of 200 annotations each for Vaibhav and I. I will begin annotations tomorrow.

\subsection{Thursday, July 14, 2022}
Today I annotated 200 posts. The following days I spent around 2 hours each day annotating 200 posts into our four categories of: culture, action, oppression, or none.

A description of the meanings of each category can be found \href{https://docs.google.com/document/d/1tLoClm225HeHbwG9vzvwQSTS5GVb8t2CusXJq6LHmEw/edit?usp=sharing}{here}

In short, here's a description of each category (examples are on the annotation guide above):

\begin{itemize}
    \item Culture: Posts attacking or causing you to dislike either Muslim or Hindu culture. 

    \item Oppression: Oppressive-provoking posts state facts about the horrors that one person/group has done against another.

    \item Action: Tells the READER of the post to do something bigger than just sharing the post, such as to stop worshiping a God. 

    \item None: posts that don't fall into any other category
\end{itemize}


\subsection{Friday, July 15, 2022}
Today I annotated 200 posts.

\subsection{Saturday, July 16, 2022}
Today I annotated 200 posts.

\subsection{Sunday, July 17, 2022}
Today I annotated 200 posts.

\section{Week of July 18, 2022}
\subsection{Monday, July 18, 2022}
Today I annotated 200 posts.

\subsection{Tuesday, July 19, 2922}
Today I annotated 200 posts.

\subsection{Wednesday, July 20, 2022}
Today I annotated 200 posts. Today we also met with Dr. Singh, the PI of the lab at NCSU. Here, we decided that we will probably be submitting our paper to the ICWSM conference. He also gave us a couple suggestions:

\begin{itemize}
    \item Trying to figure out the relationship between provoking sentences and if its indicative of the post. (NOTE: all sentences (provoking and non-provoking) were extracted from fearful posts, so this may not be a very good indication of how well provoking sentences predict fearful/not-fearful)

    \item Figure out how fearful/non-fearful posts is indicative of the type of provoking sentence.
\end{itemize}

\subsection{Thursday, July 21, 2022}
Today I annotated 200 posts.

\subsection{Friday, July 22, 2022}
Today I annotated 200 posts.

\subsection{Saturday, July 23, 2022}
No progress today.


\subsection{Sunday, July 24, 2022}
Today I annotated 400 posts.

\section{Week of July 25, 2022}
\subsection{Monday, July 25, 2022}
Today I annotated 200 posts.

\subsection{Tuesday, July 26, 2022}
No progress today.

\subsection{Wednesday, July 27, 2022}
Today I annotated 200 posts.

\subsection{Thursday, July 28, 2022}
No progress today.

\subsection{Friday, July 29, 2022}
No progress today.

\subsection{Saturday, July 30, 2022}
Today I annotated 200 posts.

\subsection{Sunday, July 31, 2022}
No progress today.

\section{Week of August 1, 2022}
\subsection{Monday, August 1, 2022}
Today I have finished annotating a fill in posts. Also, I wrote a script that combines all the annotated datasets together.

\subsection{Tuesday, August 2, 2022}
Today I have wrote a script to determine the length / sentiment of sentences in each category. The oppression category seems to have the most negative sentences, and the none category seems to be the most neutral. Posts with none are also the shortest. \ref{fig:ganning_dist} is the distribution of my annotations.

\subsection{Wednesday, August 3, 2022}
No progress today, I went to the beach.

\subsection{Thursday, August 4, 2022}
No progress today, I went to get my drivers license in the morning and I have a flight to SF in the afternoon.

\begin{figure}
    \centering
    \includegraphics[scale=1]{images/ganning_dist.png}
    \caption{Caption}
    \label{fig:ganning_dist}
\end{figure}

\subsection{Friday, August 5 - Wednesday August 10}
No progress. I was in San Francisco for a hackathon, and I had to write college apps :(

\subsection{Thursday, August 11, 2022}
Today I took the 10k sentence sample that Vaibhav and I have annotated and I combined it with my annotations, so now we have 4000 annotated sentences. Additionally, I ran a model to see how well the classification performs (it is currently a Naieve Bayes model), and doesn't use BERT yet, which I will integrate into this tomorrow.


\subsection{Friday, August 12 - Sunday, August 14}
No progress. I moved into NCSSM on Sunday.

\section{Week of August 15, 2022}
\subsection{Monday, August 15, 2022}
Today I ran a BERT model on the data set that we have. For some reason, I don't think the model is training because the accuracy is always stable at around 54.38\%. I just ran a second model and achieved a balance accuracy (90\%).

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/accuracies.png}
    \caption{Model Name vs. Accuracy when trained on the 4k sentence sample.}
    \label{fig:acc_4k_sample}
\end{figure}

\subsection{Tuesday, August 16, 2022}
Today I implemented a script to model the before AND the current sentence. However, it did not seem that the accuracy nor the balanced accuracy was affected that much.

\subsection{Wednesday, August 17, 2022}
Today we did a writing workshop:

\begin{itemize}
    \item Research papers need to tell a story. Start with introduction, then conflict (how our novel idea is different from current ideas) and then introduce our solution and talk about our features
    \item Create an acrynoym for our project so it's easier for reviewers to remember
\end{itemize}

\subsection{Thursday, August 18, 2022}
Since I'm completely done with my annotations (3k), I'm currently waiting on Vaibhav to finish his 3k sentences to be annotated. Today I met with him to determine next steps and how to move forward:

\begin{itemize}
    \item Vaibhav is currently occupied with another research project and will likely get around to doing our project's annotations by 8/31
    \item Since we do have a 90\% accuracy, I need to figure out what this really means. Is it macro or micro acccuracy?
    \begin{itemize}
        \item Macro accuracy: compute accuracy for each class (since we have four classes), and then we average the accuracies for each class to get one overall accuracy

        \item Micro accuracy: don’t consider individual accuracies; treats each sample as either predicted correctly or if it was predicted wrongly (no matter which class). Take the number of correctly predicted divided by the total number
    \end{itemize}
\end{itemize}

Additionally, we came up with a list of items for me to do!

\begin{itemize}
    \item Feed in all three sentences by concatenating and feeding in three sentences at once

    \item Read through Vaibhav's pixie paper and try: XLNet, Roberta, sentence embedding approaches
    \item Figure out the differences between BERT embeddings and my current code's usage of BERT.
\end{itemize}


\subsection{Friday, August 19, 2022}
Today I refactored my giant code file called ``BERT Provocative Sentence Classification.ipynb" which contains 2 machine learning methods. I split up each method into its individual file: BERT preprocess uncased (54\% accuracy), BERT base cased (90\%).

Looking at the differences between BERT Cased and Uncased:
\begin{itemize}
    \item BERT cased keeps all text in the original format (uppercase text stays uppercase)
    \item BERT uncased useful when capitalization/accents don't really have an effect on the meaning of the text
\end{itemize}

This strikes me as a bit odd--as most of our text doesn't really matter if the text is uppercase/lowercase (which means theoreteically, BERT uncased would be better. However, BERT cased actually produces a much higher accuracy).

\subsection{Saturday, August 20, 2022}
Today I modified my BERT model to take into account both the ``before" sentence and the current sentence within a fear speech post. Essentially, when splitting the existing dataset of fear speech and non-fear speech posts, we only look at fear speech posts. Within each fear speech post, we look at every sentence individually. Then, we store data in a CSV in a way that we also store the sentence BEFORE AND AFTER the current sentence in the overall fear speech post as well, in order to provide context for the current sentence.

When plotting the model's training history, shown in figure \ref{fig:bert_cased_w_before}, we see that while the training accuracy is continuously increasing, the validation accuracy only increases slightly, which may suggest that the model is overfitting to the training data and is not generalizing enough.

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{images/bert_cased_w_before.png}
    \caption{Bert cased model trained on the current & before sentences}
    \label{fig:bert_cased_w_before}
\end{figure}

When doing research online, here was a list of ways to reduce overfitting on machine learning models:
\begin{enumerate}
    \item Reduce the number of layers / number of elements in hidden layers (hidden layers = layers between the first and last layer)
    \item Use dropout layers, which randomly remove certain features by setting them to zero
\end{enumerate}


\subsection{Sunday, August 21, 2022 || THE CODE WORKS IM SO HAPPY}
Today I made the model we trained yesterday to be accessible to the user. For example, the user would be able to type in a sentence and the model will make a prediction on the category that the sentence belongs to. With each input, we show a percentage representing the chance that the model is in that category

THE CODE WORKS IM SO HAPPY. Here's a couple of training sentences and their predicted categories: please note that since the dataset was trained fear speech sentences extracted from Indian WhatsApp groups, I have to feed those types of sentences into the model; none of the sentences below reflect my personal opinion and are only used for testing purposes.
\begin{itemize}
    \item ``hinduism is fake" culture: 82\% (correct)
    \item ``yesterday the children played outside" none: 98\% (correct)
    \item ``you need to stop praying in all of the mosques" action: 93\% (correct)
    \item ``the hindus killed 83 million people in 2021" oppression: 96\% (correct)
\end{itemize}

Figure \ref{fig:bert_cased_metrics} shows the metrics from my trained model.

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{images/bert_cased_metrics.png}
    \caption{Bert cased model's metrics}
    \label{fig:bert_cased_metrics}
\end{figure}

\section{Week of August 22, 2022}
\subsection{Monday, August 22, 2022}
When performing data analysis on the differences between provoking sentences vs. non-provoking sentences, I created figures \ref{fig:senti_nonprov}, \ref{fig:senti_prov}, \ref{fig:len_act}, \ref{fig:len_cult}, \ref{fig:len_none}, \ref{fig:len_opp}, \ref{fig:senti_act}, \ref{fig:senti_cult}, \ref{fig:senti_none}, \ref{fig:senti_opp} (I'm honestly not sure why these references aren't working, but the graphs are shown below)

The comparison between figures \ref{fig:senti_nonprov} \ref{fig:senti_prov} shows that provoking sentences are generally more negative in sentiment than non-provoking sentences. Otherwise, thee spread of provoking/non-provoking sentences is REALLY similar, including with the small spike up at around 0.5 sentiment.

The comparison between figures \ref{fig:len_act}, \ref{fig:len_cult}, \ref{fig:len_none}, \ref{fig:len_opp} shows the lengths in comparison between the various categories. Sentences marked as ``oppression" and ``action" tend to be longer than all other categories, but the ``none" category seems to be the shortest. Average lengths are shown in figure \ref{fig:len_avg}.

The comparison between \ref{fig:senti_act}, \ref{fig:senti_cult}, \ref{fig:senti_none}, \ref{fig:senti_opp} shows that the ``none" category is essentially a peak at zero with a symmetrical distributions on either side, while all other categories show that their sentences are more negative, which is expected. Additionally, when looking at figure \ref{fig:senti_avg}, we see that oppression has the most negative sentiment, followed by culture, action, and lastly, none.

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_nonprov.png}
    \label{fig:senti_nonprov}
    \caption{Sentiment of nonprovoking sentences}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_prov.png}
    \label{fig:senti_prov}
    \caption{Sentiment of provoking sentences}
  \end{minipage}
\end{figure}

%%%%%

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/len_act.png}
    \label{fig:len_act}
    \caption{Length of action sentences}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/len_cult.png}
    \label{fig:len_cult}
    \caption{Length of culture sentences}
  \end{minipage}
\end{figure}


%%%%%

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/len_none.png}
    \label{fig:len_none}
    \caption{Length of none sentences}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/len_opp.png}
    \label{fig:len_opp}
    \caption{Length of oppression sentences}
  \end{minipage}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/len_avg.png}
    \caption{Average length by category}
    \label{fig:len_avg}
\end{figure}

%%%%%

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_act.png}
    \label{fig:senti_act}
    \caption{Sentiment of action sentences}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_cult.png}
    \label{fig:senti_cult}
    \caption{Sentiment of culture sentences}
  \end{minipage}
\end{figure}


%%%%%

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_none.png}
    \label{fig:senti_none}
    \caption{Sentiment of none sentences}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/senti_opp.png}
    \label{fig:senti_opp}
    \caption{Sentiment of oppression sentences}
  \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{images/senti_avg.png}
    \caption{Average sentiment by category}
    \label{fig:senti_avg}
\end{figure}


\subsection{Tuesday, August 23, 2022}
Today I tried to deploy my model to an API endpoint, but I kept getting an error exporting my model because it is so large. I was using the package ``joblib", which essentially allows you to download any Python variable as a file and allows you to load it into another environment and use that variable as normal. However, when attempting to load my exported file back into the code using ``joblib" I received an error saying that I was using too much RAM. I've tried debugging this error for an hour or so but no luck. I might try using pickle or using tensorflow's native export function feature next.

Additionally, I finished my presentation that I will be giving to the rest of the class on my \href{https://drive.google.com/file/d/1DS6dqwKjhULGXQlOEomzv4M6ShNxKnqK/view?usp=sharing}{Summer Updates}.

I'm also still waiting on Vaibhav to respond to me about how many annotations he has completed/his progress.

\subsection{Wednesday, August 24, 2022}
Today I spent some time thinking about the user flow of the app and figuring out how the frontend that the user interacts with would work with the backend. Figure \ref{fig:userflow_df} shows the user interactions and how that would interact with my model's frontend web app and API endpoint.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/userflow_d1.png}
    \caption{User Flow of Provocative Sentence Classification}
    \label{fig:userflow_df}
\end{figure}


\subsection{Thursday, August 25 - Sunday, August 28}
No progress these days. I was in Washington, DC to receive my award as Coolidge Senator.

\subsection{Monday, August 29, 2022}
Today in class Mr. Gotwals gave us a list of seven items that we should do:
\begin{enumerate}
    \item Keeping track of EVERYTHING: conversations, people you meet, etc
    \item Keeping a networking notebook 
    \item keeping track of all of your sources in Mendeley or another citation tracker
    \item Establishing a personal pacing guide so you know what needs to be done each day
    \item Spend more time THINKING
    \item Establish a method of evaluating and recording research papers
\end{enumerate}

Today I decided to take a break from doing my research project because I wanted a day to re-coup myself before tackling my research project. However, I've added around 5 connections from Microsoft (2), UNC RENCI (1), Credit Suisse (1), and Bit Project (1) to my connections tracker.

\subsection{Tuesday, August 30, 2022}
In class, we had a workshop with Dr. Myra Halpin on how to write our research papers for competition. Here were the 3 points that I really wanted to highlight about my research in fear speech:
\begin{enumerate}
    \item We currently know what hate speech is. In contrast, fear speech is aimed at creating a sense of existential fear in a reader (such as imminent threats)

    \item In some fear speech posts, sentences can be classified as provoking, which may cause readers to do something bad or make them angry

    \item We create a machine learning model to detect for provocative sentences with 90\% accuracy, and we’re going to make this available for everyone to use.
\end{enumerate}

Feedback about my three points:
\begin{itemize}
    \item The first sentence needs to convince the reader that fear speech and provocative sentences are an actual problem
    \item The other two points are pretty good about what I've done already and the end goal of my research (to create a provocative sentence detection model that's able to flag sentences as provocative in social media sites).
\end{itemize}

In general, these points were very important:
\begin{enumerate}
    \item The first sentence is VERY important
    \item Make what your goal is very clear
    \item Validation and how to test your data
    \item The goal isn't to sound intelligent, it is to help your audience learn and grow
\end{enumerate}

Today I took another attempt at exporting the model file that I have written in Python. However, I'm getting various errors. I'm unable to load the model back into Colab to test if it works or not since I run out of RAM each time I attempt to do that (model is too big, but was only trained on 4 epochs). I don't think ``joblib" will be able to do exporting the model. When attempting with ``pickle", I get an ``Ran out of input" error, which from Google means that the file I'm trying to read from is empty, but I've downloaded it and it clearly isn't empty...

Since these methods both didn't work, I'll be attempting to export both the model and tokenizer natively using their built in functions (which is similar to pickle and joblib).

My plan is to deploy the model on huggingface using gradio to create both a web app and an API endpoint that can identify provoking sentences.

\subsection{Wednesday, August 31, 2022}
Today I tested a version of the BERT cased model that takes in all three sentences, instead of just the one sentence. With this model, I was able to achieve 92\% balanced accuracy, as shown in figure \ref{fig:92}.

\begin{figure}
    \centering
    \includegraphics{images/92.png}
    \caption{Balanced accuracy on all 3 sentences}
    \label{fig:92}
\end{figure}

I also fed in the before and current sentence (how I annotated sentences myself), and the model produced a balanced accuracy of 97\%, as shown in figure \ref{fig:97}.

\begin{figure}
    \centering
    \includegraphics{images/97.png}
    \caption{Balanced accuracy on before and current sentences}
    \label{fig:97}
\end{figure}

\subsection{Thursday, September 1, 2022}
Today in class we went through a crash course on computational chemistry. 
Additionally, I created a pros/cons list of the MolecVue activities. Here are some PROS:

\begin{enumerate}
    \item The wording is conversational, it’s not written like a typical textbook
    \item The “Introduction and Overview” sections are really helpful in understanding what students will be doing in the lab, so one does not need to read through the entire document to understand what they will be doing.
\end{enumerate}

CONS:
\begin{enumerate}
    \item The procedure doesn’t have any pictures in it, only text descriptions of what to click. This can be confusing to find the positions of everything, especially when a teacher doesn’t know the software super well and is asked to help a student.
    \item At the end of the procedure, they don’t show a “expected” or “correct” result, which makes it difficult for students to know if the procedure they performed was correct or if they messed up somewhere.
    \item There isn’t a section on “objectives” or what the student should be learning (at a high level) at the beginning of the document. This makes it difficult for the teacher to match this document up with the standards that they need to be teaching.
    \item MolecVue doesn’t have a structured “curriculum”, it’s more of a set of activities and it can be difficult for teachers to know where to start or which activity to begin with for a group of students if they do not have previous computational chemistry experience. 
    \item Some of the documents don’t have instructors notes, such as these:
    \begin{itemize}
        \item \href{https://drive.google.com/file/d/1F03piQqm2UlihIMhjv44DLJZ301NYJzz/view}{VSEPR 5 EGs}
        \item \href{https://drive.google.com/file/d/1nUpMCmk5g_n33nR2MTbBd722eFwnzuob/view}{Group Theory BCCE}
        \item \href{https://drive.google.com/file/d/124zYFVeFI3ZVwJVfoQpoWoudQMcn7dep/view}{VSEPR 2 3 4 EGs}
    \end{itemize}
\end{enumerate}

\subsection{Friday, September 2, 2022}
Today I figured out what ``balanced accuracy" really means. Our model using BERT cased produced a 90\% accuracy. \textbf{Balanced accuracy} is the average of recall for each class in multi-class classification.

Recall that $R = \frac{T_p}{T_p + F_n}$, where $R$ is recall, $T_p$ is the number of true positives, and $T_n$ is the number of true negatives.

Today I also start my first extended weekend of senior year, so not too much was done today.

\subsection{Saturday, September 3, 2022}
Today I began working on my ISEF SRC Research Plan. I've finished the rationale, Research Questions, and hypothesis sections today.

\subsection{Sunday, September 4, 2022}
Today I've finished the first draft of my SRC research plan. I have completed the methods, risk & safety considerations, and data analysis sections today.

My research plan can be found \href{https://docs.google.com/document/d/1Q7jFEalh9ScSkIwjcvOFlRqDYMJcEpE9-Bwyo190OLA/edit#}{here}.
\section{Week of September 5, 2022}
\subsection{Monday, September 5, 2022}
I think we will now be submitting our papers to the IEEE journal on Transactions on computational social systems, Vaibhav said that there is a much higher chance of us being published there than ICWSM. However, I still want to submit to ICWSM because it is difficult and I want to give it our best shot.

The papers that Vaibhav showed me in the IEEE journal (shown in list) both seem very basic? The papers themselves don't really make sense to me, as they are basically using one model (Textblob) to extract sentiment and THEN training other models on the output data of that mode to see which model mimics the outputs the best? Isn't this basically just seeing which model can immitate the original model the best? This doesn't really make sense to me why this is useful...
\begin{itemize}
    \item COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID-19 Sentiment Analysis \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9340540}{here}
    \item Sentiment Analysis of Lockdown in India During COVID-19: A Case Study on Twitter \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9301194}{here}
\end{itemize}

\subsection{Tuesday, September 6, 2022}
Today I finished writing & turned in my ISEF research plan after going through a few rounds of revisions.

\subsection{Wednesday, September 7, 2022}
No progress today.

\subsection{Thursday, September 8, 2022}
Today I met with my mentor to discuss how to move forward. Here were the main talking points:
\begin{itemize}
    \item Vaibhav is currently working on finishing his annotations; and I will be working on data analysis on the dataset that I've created by creating (in order of priority):
    \begin{enumerate}
        \item Topic modeling (LDA) for each category (extracts top 5-10 topics)
        \item Create empath value per category
        \item LiWc lexicon for emotions per category
        \item Create a word cloud for each category (make them into a funny shape) like prepvious papers
        \item Create Toxicity per category
    \end{enumerate}
    \item Our paper will have 3 main contributions:
    \begin{enumerate}
        \item Dataset
        \item Data analysis of our each category of dataset
        \item Classification algorithm
    \end{enumerate}
    
    \item I need to: make graphs about sentiment I’ve made, calculate the skewness and kurtosis for each category
    \item Read about lime (analysis about our model)
    \item Create a flowchart to describe our methods
    \item We're going to try and get published in the ieee transactions on computational social systems journal
    \item I'll be starting the final paper by working on it a little everyday
\end{itemize}

\subsection{Friday, September 9, 2022}
Today I performed many of the data analysis items we agreed upon yesterday:
\begin{itemize}
    \item Topic modeling using gensim in Python
    \item Empathy values using empath
\end{itemize}

It's a little difficult to visualize the empath values, as the graph that the library produces is very hard to read, as shown in figure \ref{fig:empth}.
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/empth.png}
    \caption{Empth values vs. category}
    \label{fig:empth}
\end{figure}

\subsection{Saturday, September 10, 2022}
Today I continued working on the data analysis tasks:
\begin{itemize}
    \item Toxicity using the perspective API
    \item LiWc?
\end{itemize}

I requested and gained access to Google's Perspective API, which is often used to detect for toxicity. However, when testing my code, I accidentally sent too much data, so now I'm being rate limited and will need to run the actual data analysis on toxicity again tomorrow (but the code itself is done, I just need to click run again).

Additionally, I'm not really sure what I'm supposed to be doing with LiWc, which is a proprietary item? I've asked Vaibhav abobut what I'm supposed to be using it for!

\subsection{Sunday, September 11, 2022}
Today I created the charts and data analysis for toxcicity levels using the perspective API from google. However, something weird was happening with selecting random items from a list, I entered 60 random items but I kept getting back like 71? Not really sure what happened, but it didn't affect my results, and the perspective API didn't rate limit 71 posts.

Additionally, I started writing my final paper in the IEEE format to submit. I've written part of the introduction.

\subsection{Monday, September 12, 2022}
Today I met with Vaibhav to discuss our next steps, since I've finished all the data analysis tasks from the previous week. Here is what I will complete before Saturday:
\begin{itemize}
    \item Remove ``Hindu" and ``Muslims" from wordclouds and remake it
    \item Find more keywords for LDA (instead of just five)
    \begin{itemize}
        \item Extract five topics instead of just three
    \end{itemize}
    \item Add these graphs (and captions) to the overleaf document (when the above is done)
    \item Create a separate section for analysis in the overleaf document
    \item Make another section in the overleaf document and put everything I have BERT (write about how I implemented it and stuff)
    \item Try SimpleTransformer library (very simple) with XLNet, Roberta, BERT
    \item Add Simpletransformer stuff to these as well
\end{itemize}

\subsection{Tuesday, September 13, 2022}
Today I completed my RCompSci visual abstract, which is shown in figure \ref{fig:vis}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{images/vis.png}
    \caption{RCompSci visual abstract}
    \label{fig:vis}
\end{figure}

\subsection{Wednesday, September 14, 2022}
Today we had a workshop on concise sentence writing in class. My main takeaways:
\begin{itemize}
    \item It is okay to use first person in modern science writing. However, use it sparingly and only use the word ``we", not ``I".
    \item THE POINT OF THE PAPER IS TO MAKE THE READER FEEL SMARTER. IF THE READER DOESNT FEEL SMARTER YOURE NOT MAKING THEM LIKE YOUR PAPER
    \item Using big big words does not make your paper any better than if you used the simple versions of these big words
\end{itemize}

\subsection{Thursday, September 15, 2022}
Today I wrote more about the final paper and added the references I used to the bibliography. We are using a template from the IEEE. I was also helping Isha today with machine learning and her task of predicting the rich club coefficient(?) using the moca score? I suggested that she could either use a simple method like linear or logistical regression, or she could use a more advanced technique called k-nearest neighbors. 

\subsection{Friday, September 16, 2022}
I don't recall when we had this workshop, but I will be writing my notes from the data visualisation workshop with Dr. Eric Monson, Duke, Effective Data Visualization here:
\begin{itemize}
    \item Use nonsaturated colors when showing data that you're not trying to highlight or is less important
    \item Don't use big and bold black lines for tables or in graphs, they distract from the data itself
    \item Use the title of your figure, table, or graph to say what you want the reader to take away, not just like ``acceleration vs. time". Be like ``Acceleration is highest when $t=2$"
    \item You only have a very limited time before the reader loses attention
    \item Your goal is not to make yourself seem smarter, it's to make the reader feel smarter after reading about your research. If you make the reader/judge feel dumb, they're not going to like your project.
\end{itemize}

\subsection{Saturday, September 17, 2022}
Today I created a diagram that documents the process by which sentences were classified. This diagram is shown in \ref{fig:categories}.

\subsection{Sunday, September 18}
Just saw the email saying that I will be presenting at the Morganton RSci Interest Meeting. I will be talking about RCompSci, my work in RCompSci, and how research has positively impacted me. I started on the notes and slide that I will be sharing during the meeting today.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/categories.png}
    \caption{Categories by which sentences were classified}
    \label{fig:categories}
\end{figure}

\section{Week of September 19, 2022}

\subsection{Monday, September 19, 2022}
Today I worked on the paper and also got through most of the tasks Vaibhav said I should get done by the next time we meet. Vaibhav still isn't done with his annotations yet, so I'm hesitant to start creating the machine learning model, as the balance and distribution of the data will change, affecting which type of model we create. I wrote the data collection & annotation section of our paper, and revised the introduction section.

\subsection{Tuesday, September 20, 2022}
Today I have finished up revising the data collection & annotation section of the final paper. I'm also currently working on going through the graphics that I've made and changing them to be more friendly to the eyes in their colors. I'll be standardizing the color I'm using for each category in ALL graph to be the following:
\begin{itemize}
    \item None: \#a3a3a3
    \item Oppression: \#edd5a6
    \item Culture: \#8a69a0
    \item Action: \#b35656
\end{itemize}

\subsection{Wednesday, September 21, 2022}
Today I wrote a 270-word abstract for the Davidson Fellows Application. This abstract is linked \href{https://docs.google.com/document/d/1Oml89K4HLl-hOmNRIxNZvUlTRzkaLN7PjgQ6yiHBL-A/edit}{here}. I also emailed Davidson Fellows because my application portal is broken and won't let me access or view my application.
\\

Additionally, I also went through all my data analysis charts today and standardized the colors for each category and the sizes for fonts and everything.

\subsection{Thursday, September 22, 2022}
Today in class we discussed about what we were going to do for every thursday in RCompSci this fall, because we are dropping the Computational Chemistry project idea. The best ideas we came up with are:
\begin{item}
    \item Guide to RCompSci: time management, how to decide on a project, how to pace yourself, how to write a proposal (in an informal way) (voted on my majority)
    \item Data Science workshops for schools across NC (this project might take up too much time)
    \item Book discussions (not voted upon by the majority)
\end{item}

Additionally, Vaibhav has finished everything but his last 200 annotations today!

\subsection{Friday, September 23, 2022}
Today I wrote the short introduction for my research project using the following structure:

\begin{enumerate}
    \item Context
    \item Question/Problem-Condition
    \item Significance/Problem-Consequence
    \item Claim
\end{enumerate}

Here's the introduction that I wrote:
\begin{quote}
In recent years, millions of hate speech posts have been sent across social media channels each year. Along with this are thousands of fear speech messages–which aim to instill an existential fear in the reader. In contrast to hate speech, fear speech has very limited previous work. Preliminary models for fear speech detection are insufficient: there lacks a detection system for dangerous or provocative sentences within fear speech posts. Without a method of accurately detecting provocative sentences, anger and violence may ensue from those who act upon the content of these sentences. By curating a dataset and training a BERT model, an efficient fear speech detection system was created with 97\% balanced accuracy, allowing social media companies to flag these provocative sentences accordingly.
\end{quote}

Additionally, I trained a ROBERTA and another BERT model,and got around 67\% accuracy.

\subsection{Saturday, September 24, 2022}
Today I met with Vaibhav about the status of our project, and here were the updates:
\begin{itemize}
    \item We are now pivoting our project to be submit to The Web Conference
    \item Determine which type of accuracy simpletransformers are calculating
    \item Get precision and recall (for each class) onto our model in addition to accuracy
    \item Try XLNet, Universal Sentence Encoder, and other models from SimpleTransformers
    \item By next Friday (9/30), have a model with good precision/recall and have substantial information in our paper's draft
    \begin{itemize}
        \item Precision: $>= 70\%$
        \item Recall $>= 80\%$ 
    \end{itemize}
\end{itemize}

Also, we need to figure out a way to upload all of our Goolge Colab files to GitHub so we can link our code. Tutorial is linked \href{https://www.geeksforgeeks.org/how-to-upload-project-on-github-from-google-colab/}{here}.

\subsection{Sunday, September 25, 2022}
Today I had a brief meeting with Vaibhav and we have determined that IF our mdoel is good enough, we will be submitting to the web conference; however, if our model's precision and recall aren't as high as we hoped (values shown above), then we will submit to IEEE.

Today I also found a way to get recall and precision scores with simpletransformers. 

\begin{itemize}
    \item Roberta (5 epochs): precision = recall = 0.65 (only feeding in ``sentence")
    \item Roberta (10 epochs): precision = recall = accuracy = f1 = 0.685857 (only feeding in ``sentence")
    \item Roberta (3 epochs): precision = recall = accuracy = f1 = 0.693 (both ``before" and ``sentence")
    \item Roberta (10 epochs): precision = recall = accuracy = f1 = 0.7121 (both ``before" and ``sentence")
\end{itemize}

The method that accuracy is computed when using simpletransformers is totalCorrectPred / totalRows, not taking into account how many are correct by each class, only total.

\section{Week of September 26, 2022}
\subsection{Monday, September 26, 2022}
Today I wrote the introduction to my research paper and submitted it on Canvas. Vaibhav still has not finished his annotations yet though. But, I think we are making good progress towards our IEEE deadline.

\subsection{Tuesday, September 27, 2022}
Today I created a method of getting the separate precision and recall scores given a certain model and specifiying how many epochs to train it for. This is a HUGE improvement to copying and pasting the code over and over again. Now, it's much easier to test different models WITHOUT erasing the results of the previous test runs, allowing us to see which model produces the best results just by running one line of code. 

Today I also ran three models to compare accuracy. Figure \ref{fig:twitter_roberta} shows the results from a Twitter trained Roberta base model. This model shows that the precision and recall scores are VERY high, probably because of a data imbalance. We'l need to balance our model's weights before training next time. Also, the twitter roberta model was trained over 10 epochs

I also trained a ``distilbert-base-uncased-finetuned-sst-2-english" model, but received similar results.

\begin{figure}[!hbt]
    \centering
    \includegraphics[scale=0.3]{images/twitter_roberta.png}
    \caption{Twitter Roberta base model trained on 10 epochs results}
    \label{fig:twitter_roberta}
\end{figure}

\subsection{Wednesday, September 28, 2022}
Today Vaibhav finished his annotations. Additionally, I talked to Vaibhav about the low precision/recall scores, and he informed me that it is most likely because our datset is imbalanced (there are a lot more sentences labeled ``none") than any other category. Therefore, we need to add balanced classes into our model. However, while there's a way to do that using sklearn, I haven't found a method of doing balanced classes when using simpletransformers. Therefore, we can only balance our scores, and NOT our model, which seems a little off.

\subsection{Thursday, September 29, 2022}
Today we conducted a peer review of each other's introductions. I think that my introduction/research is a little difficult to understand as a whole, but I have the following changes to make to my intro:

\begin{itemize}
    \item Add a graphic to make the introduction more interesting to read
    \item FIX THE CITATIONS IN THE ITNRODUCTION. Use citep instead of just cite.
    \item Make the second example box smaller, because it is taking up a lot of room. I think there are a couple of newlines in the second box that we can definitely remove though.
\end{itemize}

\subsection{Friday, September 30, 2022}
No progress today; we moved from NCSSM \rightarrow home because of hurricane Ian.

\subsection{Saturday, October 1, 2022}
Today I wrote a script to combine Vaibhav's and I's datasets. WE HAVE OUR FULL 7K dataset now!

\subsection{Sunday, October 2, 2022}
Today I re-ran the data analysis colab files with the new data from Vaibhav's annotations. I've added those to our final IEEE paper, along with some accompanying descriptions of each. Today I had a MASSIVE headache, so I didn't get that much done.


\section{Week of October 3, 2022}

\subsection{Monday, October 3, 2022}
I retrained our BERT model to include Vaibhav's data as well today. Figure \ref{fig:bert_metrics} and \ref{fig:bert_training} shows the training and metrics data.

\begin{figure}[hbt!]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/bert_metrics.png}
    \label{fig:bert_metrics}
    \caption{Metrics of BERT model after being trained on 6500 rows of data and tested on 2000 rows random sample from the dataframe}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{images/bert_training.png}
    \label{fig:bert_training}
    \caption{Training history of the BERT model}
  \end{minipage}
\end{figure}

Additionally, Figure \ref{fig:bert_history} shows the loss and accuracy progression of the model.

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/bert_history.png}
    \caption{Loss and accuracy progression of the model}
    \label{fig:bert_history}
\end{figure}

Today I also tried to use 10-fold cross-validation to remedy the class imbalance problem. However, the precision and recall scores were still quite low, after using 10-folds, 7 epochs on each fold, and received the following results, as shown in figure \ref{fig:ten_folds}

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{images/ten_folds.png}
    \caption{10-fold cross validation for model}
    \label{fig:ten_folds}
\end{figure}

\subsection{Tuesday, October 4, 2022}
Today we used an XLNet cased model, an autoregressive Transformer that's comparable to BERT, and received really good results. We also did over-sampling of our dataset to do this, because our dataset was imbalanced, with ``none" having 3923 rows. Thus, we oversampled the rest of the rows using df.sample(replace=True) to make sure that all other rows also had 3923 rows. This created some really good results, with precision and recall for all categories being above 0.8 (mostly above 0.9). THIS IS HUGE.

\subsection{Wednesday, October 5, 2022}
Today I ran our XLNet cased model for 10 folds of cross validation, with the random state seeded at 1337. This took around 10 hours, since the GPU on Google Colab is slower, but the results are shown in \ref{fig:summary_xlnet}.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/summary_xlnet.png}
    \caption{10-fold cross validation for XLNet base cased}
    \label{fig:fig:summary_xlnet}
\end{figure}

\subsection{Thursday, October 6, 2022}

Today I ran the XLNet model again, but with an 80/20 split on train and testing datasets. We achived pretty similar results from the 10-fold splits as before. We've also submitted our abstract to the special track at The Web Conference today!


Today I also trained a couple of more models on our dataset, including BERT and Roberta. I also got halfway done with DeBERTa's training. Today I also wrote an outline for the method section. WE ALSO SUBMITTED OUR ABSTRACT TO THE WWW CONFERENCE SPECIAL TRACK TODAY!



\subsection{Friday, October 7, 2022}
Today I finished training the microsoft/deberta-base model and I also trained a new the distilbert model. Additionally, I'm currently training an XLM-Roberta model, with the XLM-roberta being different than Roberta by the fact that XLM roberta is for multi-lingual text, which is somewhat like the text what we have in our dataset (which was from Indian whatsapp groups).

Additionally, I've created the graph in Figure \ref{fig:compare_PR} that shows a comparison of the precision and recall by model. It's important to note the axis of this graph that all of these models are REALLY GOOD.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/compare_PR.png}
    \caption{Comparison of the precision and recall by model}
    \label{fig:compare_PR.png}
\end{figure}

I've also written a good bit of our research paper today, but I'm not following the guideliens of methods, results, etc, Im more talking about what we did for data analysis, model development, testing, etc. Im doing this because this makes more sense to me with how the paper is organized and such.

\subsection{Saturday, October 8, 2022}
Today I revised the computational tools, data collection, data analysis, and model development sections of the rengeron paper and turned it in. I didn't know how to add the LDA topic extraction, as I'm not sure which heading each set of keywords extracted falls under. I'd have to talk to my mentor about that. However, I did turn in my methods draft!

\subsection{Sunday, October 9, 2022}
No progress today. I was moving into NCSSM and had to go out with parents in the morning.

\section{Week of October 10, 2022}
\subsection{Monday, October 11, 2022}
Today we read over RCompSci applications in class. Some notable items:

\begin{itemize}
    \item Most good applicants already have a research idea \& write more
    \item Most weak applicants did not have a reserach idea \& wrote shorter
    \item It's good to brag about what you've done
\end{itemize}

\subsection{Tuesday, October 12, 2022}
Today we read over the rest of the RCompSci applications. I also emailed the WWW conference folks to claritfy about the deadline for our research paper for the Web4Good track, and got the answer that the full paper for the Web4Good track is due on 11/18, so I have a bit of time before we actually need to submit it.

\subsection{Wednesday, October 13, 2022}
Today I wrote a script that classified sentences from our 7k annotated dataset using MultinomialNB, LogisticRegression, RandomForest, and LinearSVC methods. As expected, these simplistic methods did not perform nearly as well as the transformer based ones that we used. Across these simplistic methods, the recall score for ``none" was generally the highest, while ``action" was the type that was least detected correctly. The resutls can be seen in figures \ref{fig:simp_linear}, \ref{fig:simp_multi}, \ref{fig:simp_forest}, and \ref{fig:simp_logistic}.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/simp_linear.png}
    \caption{Classification metrics of Linear SVM}
    \label{fig:simp_linear}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/simp_forest.png}
    \caption{Classification metrics of Random Forest}
    \label{fig:simp_forest}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/simp_logistic.png}
    \caption{Classification metrics of Logistic Regression}
    \label{fig:simp_logistic}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/simp_multi.png}
    \caption{Classification metrics of Multinomial Naive Bayes}
    \label{fig:simp_multi}
\end{figure}

\subsection{Thursday, October 14, 2022}
Today I got a couple of questions clarified by Mr. Gotwals:

\begin{enumerate}
    \item Regeneron papers have a specific format that they need to follow, the regeron template has that format
    \item However, multiple "columns" can be used when the graphics are talking about the same topic
    \item I can submit the paper to the WWW conference, however, I'll just need to write two papers in that case, one for regeron and one for WWW.
\end{enumerate}

I ended up grouping a lot of my graphics together, and now the page count on my regeron paper is a lot less. Placing graphs side by side in a single Figure saves a lot of room when they are talking about basically the same thing.

\subsection{Friday, October 15, 2022}
Today I began working on my Regeneron Application. Also I resized a lot of the graphics in my regeneron paper to be a fixed size (10, 6), which words best. I've combined certain graphics into one to save room as well

\subsection{Saturday, October 16, 2022}
I began transferring the data from the non-transformer models to create a chart of the precision and recall scores by category for both the transformer based models and the non-transformer based models.

\subsection{Sunday, October 17, 2022}
Today I finished creating the graphic that I described yesterday. This finished figure can be seen in figure \ref{fig:including_non_trans}. This graphic clearly shows the difference between precision and recall of transformer-based and non-transformer based models.

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/including_non_trans.png}
    \caption{Caption}
    \label{fig:including_non_trans}
\end{figure}

\section{Week of October 18, 2022}
\subsection{Monday, October 18, 2022}
Today I wrote my results section (which was basically my model section, as that is the main result of our paper). I then revised this and turned it in.

\subsection{Tuesday, October 19, 2022}
Today I worked on my Regeneron application, specifically the questions about contributions to the project and activities list.

\subsection{Wednesday, October 20, 2022}
Today I wrote more of my regeneron application, (essays section), and uploaded our dataset to GitHub. I won't be uploading our code from the research to GitHub yet, because the Goolge Colab files aren't done yet, but I will do that evnetually. I'm also considering adding the model summary files to the GitHub as well.

\subsection{Thursday, October 21, 2022}
Today I met with Vaibhav to discuss our next steps. The list we came up with is:
\begin{itemize}
    \item Create f1 score (forumla can be found with precision and recall calcuations
    \item Create macros for f1, recall, and precision by averaging each "4 categories"
    \item Use SVC instead of just linear SVM, which is purely a varient of SVC.
    \item Use different word embedding methods like tfidf, word2vec
\end{itemize}


\subsection{Friday, October 22, 2022}
Today I wrote a script that utilizes different word embedding method and runs all four models we have on it. Today I did tfidf, but I also realized that SVM will not work as it clogs up Google Colab's RAM, so I must use LinearSVC. 

\subsection{Saturday, October 23, 2022}
Today I ran that same script but with word2vec. I attempted to use Glove and Universal sentence encoder, but I could not find any tutorials online that used these methods and still worked. 

\subsection{Sunday, October 23, 2022}
Today I aggregated the data and produced macro plots. FOr example, \ref{fig:macros} shows the plot of recall, precision, and f1 scores macros across all models.


\begin{figure}
    \centering
    \includegraphics[scale=0.4]{images/macros.png}
    \caption{Precision, Recall, and F1 score macros across all models. The transformer models can clearly be seen outperforming the other ones, but the TFIDF with linearSVC performs the best on the others.}
    \label{fig:macros}
\end{figure}

\section{Week of October 24, 2022}
\subsection{Monday, October 24, 2022}
Today I finished my Regeneron paperwork (all my essays, activities, requesting recommendation, etc). I also wrote a little bit more about our paper.

\subsection{Tuesday, October 25, 2022}
Today I wrote the discussion/conclusion section of my paper. However our paper right now is way short of the 20 page limit, so I'm going to be thinking of ways to make our paper longer:
\begin{itemize}
    \item Including separate graphics for transformer and non-transformer based models
    \item Including more specifics in data analysis on each sub-category of provocative sentences
\end{itemize}


\subsection{Wednesday, October 26, 2022}
Today I went through and revised my regeneron application. I revised my essays section. I also read over and revised a little bit of the current regeneron paper I have.

\subsection{Thursday, October 27, 2022}
Today I spent a long time trying to fix the references in my regeneron paper. However, I wasn't able to fix it and I sent it to Mr. Gotwals. I think the issue ended up being because of a troublesome package called natbib. Once we removed that import, the error message went away.

\subsection{Friday, October 28, 2022}
No progress today. Today is the beginning of the extended weekend and I moved from NCSSM back home.

\subsection{Saturday, October 29, 2022}
Today I revised the introduction and computational tools section of my research paper for regeneron.

\subsection{Sunday, October 30, 2022}
Today I re-created the graphics for each category that would be used. These graphics are two columns wide for toxicity and length. 

\section{Week of October 31, 2022}
\subsection{Monday, October 31, 2022}
Today I revised all sections of my research paper up until the model development section (not included) :(. I only have discussion and model development to go.

\subsection{Tuesday, November 1, 2022}
Today I finally... FINALLY FINALLY finished my first round of revisions on my regeneron paper. I never knew how long this process would actually take, and wow am I glad I have finished the first round of them. I had to add so many graphics and change the wording in so many places :(.

\subsection{Wednesday, November 2, 2022}
Today I plugged my entire regeneron paper through grammarly and sent the finished draft to Mr. Gotwals. Waiting on the the feedback now!

\subsection{Thursday, November 3, 2022}
I met with my mentor yesterday about our research project, and the main concern right now is authorship:
\begin{itemize}
    \item Mentor says that he wants first author of our research paper when we submit to The Web Conference (WWW). Mentor's argument:
    \begin{itemize}
        \item However, he said that I can have first author for Regeneron
        \item He came up with the idea for the project (he says his lab's culture says this means he gets authorship)
    \end{itemize}
    \item DATASET: We each annotated half of the dataset
    \item DATA ANALYSIS: I wrote all code used 
    \item MACHINE LEARNING: I wrote all code used (also trained on my computer)
    \item PROJECT IDEA: I read many research papers and found an interesting one to show to Vaibhav. He came up with the overall research idea from that
    \item CONCLUSIONS: I came up with the discussion/conclusion and wrote them out
    \item RESEARCH PAPER: I wrote the entire Regeneron paper
\end{itemize}

I strongly believe that I should be the one getting authorship for this research project, and that my mentor is pretty much taking advantage of me since I'm a high school student (for him to get first author).

Today I also got Universal Sentence Encoder working; it's currently running the K-Fold cross validation on my compute! Universal sentence encoder seems to work REALLY well, especially when paired with LinearSVC. It's just a tiny bit off of TFIDF paired with LinearSVC.

I also updated our Regeneron paper to include the changes that we made! I also updated the \href{https://github.com/ganning127/identifying-provocative-sentences}{github repository} with the code we used throughout this project. 

\subsection{Friday, November 4, 2022}
Today I met with Mr. Gotwals about the authorship dilema I am facing. It kind of sucks that he wants first author. It should be who does the most work on the project. Mr. Gotwals will ask Holden Thorp about it. This may be one where I'll just take it for the team, since Vaibhav is a PHD student. 

I could make the argument that it's important for me to get a first author paper out as well (but much harder to make this argument). Tell Vaibhav Mr. Gotwals will consult with somebody to get an opinion.

\subsection{Saturday, November 5, 2022}
Today I revised a couple of section of my regeneron application: research study, test scores, rules, previous research, science research description, basic information. I put that I am using human data for one part of it (I don't need IRB approval). The box I checked was ``F. I used data generated from humans that I did NOT collect from them directly (obtained from a database, twitter, mentor or other source)."

\subsection{Sunday, November 6, 2022}
Today I revised the more difficult parts of my Regeneron application: especially all the essays. This took much longer than expected. I also revised the ``Beyond the project setion". I got my revised RCompSci paper back today, and luckily, there isn't that much I need to revise!

\section{Week of November 7, 2022}
\subsection{Monday, November 7, 2022}
Today I revised my Regeneron paper with the edits from Mr. Gotwals. I think my Regeneron application is almost ready to submit! Today I also talked to Vaibhav about the topic of authorship, and this is what we decided on:
\begin{itemize}
    \item We are no longer going to submit to the WWW conference, we are going to submit to IEEE transactions journal instead, because Dr. Singh didn't want us to submit a short paper.
    \item Since IEEE transactions allows first authorship sharing, that means we are going to share the first author
\end{itemize}

\subsection{Tuesday, November 8 2022}
Today I revised my entire research paper again; I went through each section and changed the sections that were confusing / didn't make sense. I think it's good now, and it's at 17 pages. 

\subsection{Wednesday, November 9, 2022}
REGENERON HAS BEEN SUBMITTED BABYYYYYYYYYYYYYYYYYYY. I've also submitted the final paper for this semester as my regeneron paper.

\subsection{Thursday, November 10, 2022}
No progress today. We had our fresh meat party for the upcoming juniors, and I ate five slices of pizza and one slice of cake.

\subsection{Friday, November 11, 2022 - Sunday, November 13, 2022}
No progress on these days. I'm waiting to start my research poster next Monday!

\section{Week of November 14, 2022}
\subsection{Monday, November 14, 2022}
Today I uploaded the full 25k sentence dataset with individual sentences to GitHub and added a citation to the original paper who produced and collected the Fear Speech WhatsApp data

Today I also began working on my research poster for this class. My poster will be created using better poster, but in Canva. Figure \ref{fig:better} shows the current state of my better poster.

\begin{figure}[!hbt]
    \centering
    \includegraphics[scale=0.4]{images/better.png}
    \caption{better poster as of 11/14}
    \label{fig:better}
\end{figure}

\subsection{Tuesday, November 15, 2022}
Today I created the first complete draft of my RCompSci final poster. This is shown in Figure \ref{fig:draft1_poster}. Today I also wrote one of my parts in the RCompSci manual. I also wrote my other part, about writing the research paper, into the RCompSci manual.

\begin{figure}[!hbt]
    \centering
    \includegraphics[scale=0.1]{images/draft1_poster.png}
    \caption{First draft of complete poster}
    \label{fig:draft1_poster}
\end{figure}

\subsection{Wednesday, November 16, 2022}
Today I added to the RCompSci manual and created another draft of my research paper and sent it to Mr. Gotwals for feedback. This is shown in Figure \ref{fig:draft2_poster}. 


\begin{figure}[!hbt]
    \centering
    \includegraphics[scale=0.1]{images/draft2_poster.png}
    \caption{First draft of complete poster}
    \label{fig:draft2_poster}
\end{figure}

\subsection{Thursday, November 17, 2022}
Today I made more edits to my research poster using Mr. Gotwals' feedback. we've been mainly changing the text in the center of the poster, because our claim for the project is that provocative sentences are different, but just as dangerous. BUT ALSO, our results show that we made a new dataset and trained multiple machine learning models. Today I also started working on my \href{https://docs.google.com/presentation/d/1spHO8QKtwy8Nd1oF7E_m8MraOz_HqJvcrgR8pfkyHd4/edit?usp=sharing}{talk for SNCURCS}. I think my presentation theme will be very relaxed (not many graphics to distract the readers from the point of the slides), but I think I have a pretty good outline going right now. I've also submitted the RCompSci poster today!

\subsection{Friday, November 18, 2022}
No progress today. I was sick as a dog.

\subsection{Saturday, November 19, 2022}
Today I kept working on my presentation for SNCURCS.

\subsection{Sunday, November 19, 2022}
Today I finished my presentation for SNICKERS. It is now on Canva.

\section{Week of November 21, 2022}
\subsection{Monday, November 21, 2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
Note: all of my sources are in Mendeley, so I don't have a bibliography here.

\end{document}

\begin{comment}
%Sample LaTeX Code
    % \bibitem{giusti}
    % Giusti, Santochi, \emph{Tecnologia Meccanica e Studi di Fabbricazione}. Casa Editrice Ambrosiana, Seconda Edizione
    % \begin{figure}[H]\centering
    % \includegraphics[scale=0.5]{weintrop.jpg}
    % \caption{Weintrop's Taxonomy}
    % \label{fig:weintrop}
    % \end{figure}

\end{comment}
